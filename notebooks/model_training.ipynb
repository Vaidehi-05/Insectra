{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aY_DZ_SzSXQA"
      },
      "source": [
        "### DOWNLOADING AND WORKING WITH DIRECTORIES\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H2eb0GLhgyUp"
      },
      "outputs": [],
      "source": [
        "!pip install scipy\n",
        "!pip install catboost\n",
        "import scipy\n",
        "import catboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQhAnBchIAIi"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "\n",
        "from scipy import stats\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, HistGradientBoostingClassifier, AdaBoostClassifier\n",
        "from xgboost import XGBClassifier\n",
        "# from lightgbm import LGBMClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Dense, Flatten, Dropout, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJQu6_VJLG4F"
      },
      "outputs": [],
      "source": [
        "#load dataset\n",
        "df=pd.read_csv(r\"D:\\Projects\\Minor\\test\\final_insect_env_merged.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pISY6wFJYA3k"
      },
      "source": [
        "### EXPLORING DATASET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wo6dqE20LVsA",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "#top 5 rows --gives a peak into what the data feels like\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dGkE0dVGPlz"
      },
      "outputs": [],
      "source": [
        "#viewing last 5 entries of the dataset\n",
        "df.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0EUEy8BL0l-"
      },
      "outputs": [],
      "source": [
        "#some std parameters of the dataset\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jK1Z3njEMGsa"
      },
      "outputs": [],
      "source": [
        "#names of columns of dataset\n",
        "df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puAOa_TpdN1y"
      },
      "source": [
        "## DATA PREPROCESSING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OGZvMFuoIQ6"
      },
      "source": [
        "### CHECKING FOR COLUMNS WITH STRING DATATYPE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9-_pRwKoPZo"
      },
      "outputs": [],
      "source": [
        "for col in df.columns:\n",
        "  if(df[col].dtype=='object'):\n",
        "    print(col)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "craOO-_SiTkL"
      },
      "source": [
        "### REMOVING COLUMNS THAT DO NOT HAVE AN IMPACT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PD4Rgg3ibZB",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# removing 'file' column as it doesn't impact the prediction\n",
        "df=df.drop(['file'], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCyutLgqX1An"
      },
      "source": [
        "###CHECKING FOR MISSING VALUES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AiHA11sxV0EA"
      },
      "outputs": [],
      "source": [
        "#checking for missing values\n",
        "cols_with_missing=[col for col in df.columns if df[col].isnull().any()]\n",
        "print(cols_with_missing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SFAl9NYAJFJm"
      },
      "outputs": [],
      "source": [
        "df = df.drop(columns=['Unnamed: 165'], errors='ignore')\n",
        "df = df.replace([np.inf, -np.inf], np.nan)\n",
        "numeric_cols = df.select_dtypes(include=np.number).columns\n",
        "df = df.dropna(subset=numeric_cols).reset_index(drop=True)\n",
        "zero_var_cols = [c for c in numeric_cols if df[c].std() == 0]\n",
        "df = df.drop(columns=zero_var_cols)\n",
        "print(\"Dropped zero-variance columns:\", zero_var_cols)\n",
        "df[numeric_cols] = df[numeric_cols].clip(lower=df[numeric_cols].quantile(0.001),\n",
        "                                         upper=df[numeric_cols].quantile(0.999),\n",
        "                                         axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WgUPevGUXUr3"
      },
      "outputs": [],
      "source": [
        "#checking count of columns with missing values as well\n",
        "df.isnull().sum().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLsQAcLfY_SE"
      },
      "source": [
        "### CHECK FOR DUPLICATE VALUES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4dHq6CMcZED7"
      },
      "outputs": [],
      "source": [
        "#checking for duplicate values\n",
        "df.duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bxuY40kJpifE"
      },
      "outputs": [],
      "source": [
        "#removing duplicated data\n",
        "df=df.drop_duplicates().reset_index(drop=True)\n",
        "df.duplicated().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eddta2IxZuKh"
      },
      "source": [
        "### CHECKING FOR OUTLIERS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6k3DKSiBahVK",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "\n",
        "# InterQuartile Range Check\n",
        "numeric_cols=df.select_dtypes(include=np.number).columns # taking only numeric columns\n",
        "\n",
        "outliers={}\n",
        "\n",
        "for col in numeric_cols:\n",
        "  Q1=df[col].quantile(0.25)\n",
        "  Q2=df[col].quantile(0.75)\n",
        "  IQR=Q2-Q1\n",
        "  lower=Q1-1.5*IQR\n",
        "  upper=Q2+1.5*IQR\n",
        "  outliers[col]=df[(df[col]<lower)|(df[col]>upper)][col].count()\n",
        "\n",
        "#show outliers\n",
        "{k:v for k,v in outliers.items() if v>0}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZ9aYHMtf7nh"
      },
      "outputs": [],
      "source": [
        "# Z-score outlier detection\n",
        "from scipy import stats\n",
        "z_scores=np.abs(stats.zscore(df[numeric_cols]))\n",
        "outlier_rows=np.where(z_scores>3)[0]\n",
        "len(outlier_rows)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzQ_tl89UiC0"
      },
      "source": [
        "### CHECKING CLASS DISTRIBUTION\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIs7E387UqQr"
      },
      "outputs": [],
      "source": [
        "# Checking counts in individual categories\n",
        "counts=df['label'].value_counts()\n",
        "counts\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QzzIAfwv4j25"
      },
      "outputs": [],
      "source": [
        "df['label'].value_counts(dropna=False)\n",
        "df[df['label'].isna()].head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CyGLpSJnYSAd"
      },
      "outputs": [],
      "source": [
        "# visualising the distributions\n",
        "counts = df['label'].value_counts().reset_index()\n",
        "counts.columns = ['label', 'count']\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.barplot(data=counts, x='label', y='count', palette='viridis')\n",
        "\n",
        "plt.title(\"Original Dataset Class Distribution\", fontsize=18)\n",
        "plt.xlabel(\"Class Label\", fontsize=14)\n",
        "plt.ylabel(\"Count\", fontsize=14)\n",
        "plt.xticks(rotation=20, ha='right', fontsize=12)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3elbe1PmqnlZ"
      },
      "source": [
        "## DATA BALANCING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlpr4utxq7QX"
      },
      "outputs": [],
      "source": [
        "# Seperating classes acc. to label\n",
        "others_df=df[df['label']==\"Others\"].copy()\n",
        "env_df=df[df['label']==\"Env\"].copy()\n",
        "species_df=df[~df['label'].isin([\"Others\", \"Env\"])].copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnNTZ2ZXn5OC"
      },
      "source": [
        "### CLUSTERING BASED SAMPLING FOR 'OTHERS'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNQn4Isan9TG"
      },
      "outputs": [],
      "source": [
        "#keeping all numeric columns\n",
        "feat_cols=[c for c in df.columns if c not in['label']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBzs-C44qJGG"
      },
      "outputs": [],
      "source": [
        "# scaling numeric values for uniformity\n",
        "scaler=StandardScaler()\n",
        "scaler.fit(df[feat_cols])\n",
        "X_others = scaler.transform(others_df[feat_cols])\n",
        "X_env    = scaler.transform(env_df[feat_cols])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K68K9QghrFdv"
      },
      "outputs": [],
      "source": [
        "# Applying KMeans\n",
        "RANDOM_STATE=42\n",
        "n_clusters_others = 35\n",
        "n_clusters_env    = 25\n",
        "kmeans_others = MiniBatchKMeans(\n",
        "    n_clusters=n_clusters_others,\n",
        "    batch_size=1024,\n",
        "    random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "kmeans_env = MiniBatchKMeans(\n",
        "    n_clusters=n_clusters_env,\n",
        "    batch_size=1024,\n",
        "    random_state=RANDOM_STATE\n",
        ")\n",
        "others_clusters = kmeans_others.fit_predict(X_others)\n",
        "env_clusters    = kmeans_env.fit_predict(X_env)\n",
        "\n",
        "# Assign cluster IDs (aligned indices)\n",
        "others_df['cluster'] = others_clusters\n",
        "env_df['cluster']    = env_clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AA11FIu6r3QA"
      },
      "outputs": [],
      "source": [
        "# Sampling individual categories\n",
        "def proportional_sample(df_sub, cluster_col, target_n, random_state=42):\n",
        "    clusters = df_sub[cluster_col].unique()\n",
        "    samples = []\n",
        "    total = len(df_sub)\n",
        "\n",
        "    for cl in clusters:\n",
        "        cluster_df = df_sub[df_sub[cluster_col] == cl]\n",
        "        size = len(cluster_df)\n",
        "\n",
        "        # proportional allocation\n",
        "        n = max(1, int(round(size / total * target_n)))\n",
        "        # safe sampling\n",
        "        selected = cluster_df.sample(\n",
        "            n=min(n, size),\n",
        "            replace=False,\n",
        "            random_state=random_state\n",
        "        )\n",
        "        samples.append(selected)\n",
        "\n",
        "    sampled = pd.concat(samples).reset_index(drop=True)\n",
        "\n",
        "    # Adjust overshoot/undershoot\n",
        "    if len(sampled) > target_n:\n",
        "        sampled = sampled.sample(target_n, random_state=random_state).reset_index(drop=True)\n",
        "    elif len(sampled) < target_n:\n",
        "        needed = target_n - len(sampled)\n",
        "        remaining = df_sub.loc[~df_sub.index.isin(sampled.index)]\n",
        "        extra = remaining.sample(needed, random_state=random_state)\n",
        "        sampled = pd.concat([sampled, extra]).reset_index(drop=True)\n",
        "    return sampled\n",
        "target_others = 3000\n",
        "target_env    = 2500\n",
        "\n",
        "others_sample = proportional_sample(others_df, 'cluster', target_others)\n",
        "env_sample    = proportional_sample(env_df, 'cluster', target_env)\n",
        "# Remove temporary clustering column\n",
        "others_sample = others_sample.drop(columns=['cluster'], errors='ignore')\n",
        "env_sample    = env_sample.drop(columns=['cluster'], errors='ignore')\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDTsQLlEyeX0"
      },
      "outputs": [],
      "source": [
        "# creating the balanced dataset that'll be used further\n",
        "balanced_df = pd.concat(\n",
        "    [species_df, others_sample, env_sample],\n",
        "    ignore_index=True\n",
        ")\n",
        "balanced_df = balanced_df.sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qE_vcaN_PY5F"
      },
      "source": [
        "### CLUSTERING BASED SAMPLING FOR 'ENV'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GcNmlkLGUtSH"
      },
      "outputs": [],
      "source": [
        "# combining everything\n",
        "len(balanced_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uV8stRhxV1ou"
      },
      "source": [
        "### CHECKING IF ALL WORK DONE CORRECTLY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lii8x7dzV8ZW"
      },
      "outputs": [],
      "source": [
        "len(balanced_df)\n",
        "balanced_df.isnull().sum().sum()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Adt4-lW3WJeU"
      },
      "outputs": [],
      "source": [
        "balanced_df['label'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qszhc_THWzze"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "sns.countplot(data=balanced_df, x='label', palette='viridis')\n",
        "plt.title(\"Balanced Dataset Class Distribution\")\n",
        "plt.xlabel(\"Class Label\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.xticks(rotation=30, ha='right')   # rotate & align right\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOc9xb7HIqBH"
      },
      "source": [
        "## DATA SCALING AND ENCODING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NdY_ry6xIuly"
      },
      "outputs": [],
      "source": [
        "# keeping only numeric data\n",
        "X=balanced_df.drop(['label'], axis=1)\n",
        "\n",
        "# Target column\n",
        "y=balanced_df['label']\n",
        "\n",
        "# Splitting data for training and testing\n",
        "X_train, X_test, y_train, y_test=train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4A93FWiqkeEv"
      },
      "outputs": [],
      "source": [
        "# Scaling data using Robust Scaler\n",
        "scaler=RobustScaler()\n",
        "\n",
        "# fit only on training data\n",
        "X_train_scaled=scaler.fit_transform(X_train)\n",
        "\n",
        "#transform test data using same scaling\n",
        "X_test_scaled=scaler.transform(X_test)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5YxqjXBJFJy"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# -----------------------------\n",
        "# 1. Create a DataFrame for scaled features\n",
        "# -----------------------------\n",
        "scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
        "\n",
        "print(\"\\n===== SUMMARY STATISTICS (SCALED DATA) =====\")\n",
        "display(scaled_df.describe().T)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 2. Check for NaN / Inf values\n",
        "# -----------------------------\n",
        "print(\"\\n===== NaN / Inf Check =====\")\n",
        "print(\"NaN count:\", scaled_df.isna().sum().sum())\n",
        "print(\"Inf count:\", np.isinf(scaled_df).sum().sum())\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 3. Compare ONE FEATURE before & after scaling\n",
        "# -----------------------------\n",
        "feature = X_train.columns[0]  # choose first feature\n",
        "\n",
        "plt.figure(figsize=(12,5))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.hist(X_train[feature], bins=50)\n",
        "plt.title(f\"Original: {feature}\")\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.hist(scaled_df[feature], bins=50)\n",
        "plt.title(f\"Scaled: {feature}\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 4. Boxplot of first 50 scaled features\n",
        "# -----------------------------\n",
        "plt.figure(figsize=(14,6))\n",
        "plt.boxplot(scaled_df.iloc[:, :50], vert=False)\n",
        "plt.title(\"Scaled Feature Distribution (First 50 Features)\")\n",
        "plt.xlabel(\"Value\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 5. Inspect RobustScaler params\n",
        "# -----------------------------\n",
        "print(\"\\n===== RobustScaler Center (Median of Each Feature) =====\")\n",
        "center_series = pd.Series(scaler.center_, index=X_train.columns)\n",
        "display(center_series)\n",
        "\n",
        "print(\"\\n===== RobustScaler Scale (IQR of Each Feature) =====\")\n",
        "scale_series = pd.Series(scaler.scale_, index=X_train.columns)\n",
        "display(scale_series)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 6. Compare full feature vector for one sample\n",
        "# -----------------------------\n",
        "idx = 0  # choose first sample\n",
        "\n",
        "plt.figure(figsize=(16,5))\n",
        "plt.plot(X_train.iloc[idx], label=\"Original\")\n",
        "plt.plot(scaled_df.iloc[idx], label=\"Scaled\")\n",
        "plt.legend()\n",
        "plt.title(\"Full Feature Vector â€” Before vs After Scaling\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIiGpqcUJFJz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Convert center_ array to Series\n",
        "center_series = pd.Series(scaler.center_, index=X_train.columns)\n",
        "\n",
        "# Filter MFCC mean + std\n",
        "mfcc_centers = center_series[center_series.index.str.contains(r\"mfcc_\", regex=True)]\n",
        "\n",
        "# Filter delta means + std\n",
        "delta_centers = center_series[center_series.index.str.contains(r\"delta_mean_|delta_std_\", regex=True)]\n",
        "\n",
        "# Filter delta2 means + std\n",
        "delta2_centers = center_series[center_series.index.str.contains(r\"delta2_mean_|delta2_std_\", regex=True)]\n",
        "\n",
        "print(\"==== MFCC CENTERS (mean + std) ====\")\n",
        "display(mfcc_centers)\n",
        "\n",
        "print(\"\\n==== DELTA CENTERS (mean + std) ====\")\n",
        "display(delta_centers)\n",
        "\n",
        "print(\"\\n==== DELTA2 CENTERS (mean + std) ====\")\n",
        "display(delta2_centers)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8zTsmmPJFJz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wc6or4K4KkMz"
      },
      "outputs": [],
      "source": [
        "# converting scaled np arrays back to dataframes\n",
        "X_train_scaled=pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
        "X_test_scaled=pd.DataFrame(X_test_scaled, columns=X_test.columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ph534Q1aAH08"
      },
      "outputs": [],
      "source": [
        "# encoding data for as and when needed\n",
        "le=LabelEncoder()\n",
        "y_train_enc=le.fit_transform(y_train)\n",
        "y_test_enc=le.transform(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "me0kRjgyLgoe"
      },
      "outputs": [],
      "source": [
        "joblib.dump(scaler, \"robust_scaler.pkl\")\n",
        "# joblib.dump(scaler, \"/content/drive/MyDrive/Insectra/robust_scaler.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QrT2MAUYJFJz"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Save training column order\n",
        "with open(\"feature_order.json\", \"w\") as f:\n",
        "    json.dump(list(X_train.columns), f)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfcUgE9ZRp_-"
      },
      "source": [
        "## MODEL TRAINING AND TESTING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAXxTCBJTKiM"
      },
      "outputs": [],
      "source": [
        "#defining and declaring all the models we'll be trying\n",
        "models={\n",
        "    \"SVM-RBF\": SVC(kernel=\"rbf\", probability=True),\n",
        "    \"RandomForest\": RandomForestClassifier(n_estimators=300),\n",
        "    \"XGBoost\": XGBClassifier(n_estimators=300, learning_rate=0.05, max_depth=6, subsample=0.8),\n",
        "    # \"LightGBM\": LGBMClassifier(n_estimators=300, learning_rate=0.05),\n",
        "    \"LogisticRegression\": LogisticRegression(max_iter=500),\n",
        "    \"KNN\": KNeighborsClassifier(n_neighbors=7),\n",
        "    \"CatBoost\": CatBoostClassifier(iterations=300, learning_rate=0.05, depth=8, verbose=False, random_state=42),\n",
        "    \"ExtraTrees\": ExtraTreesClassifier(n_estimators=400, random_state=42, n_jobs=-1),\n",
        "    \"HistGradientBoosting\": HistGradientBoostingClassifier(learning_rate=0.05, max_depth=8, random_state=42),\n",
        "    \"AdaBoost\": AdaBoostClassifier(n_estimators=300, learning_rate=0.05, random_state=42)\n",
        "\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1XD28J5gfOH"
      },
      "outputs": [],
      "source": [
        "results = {}\n",
        "# storing to store individual f1 scores for each category and using each model\n",
        "f1_results = []\n",
        "\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\n----------------------\")\n",
        "    print(f\"Training {name}...\")\n",
        "\n",
        "    if(name in ['XGBoost', 'LightGBM', 'HistGradientBoosting', 'AdaBoost']):\n",
        "      model.fit(X_train_scaled.values, y_train_enc)\n",
        "      preds = model.predict(X_test_scaled.values)\n",
        "      preds = le.inverse_transform(preds)  # convert back to strings\n",
        "    else:\n",
        "      model.fit(X_train_scaled, y_train)\n",
        "      preds = model.predict(X_test_scaled)\n",
        "      # acc = accuracy_score(y_test, preds)\n",
        "\n",
        "    acc = accuracy_score(y_test, preds)\n",
        "    print(f\"{name} Accuracy: {acc:.4f}\")\n",
        "    print(classification_report(y_test, preds))\n",
        "    # store per-class F1 scores\n",
        "    report = classification_report(y_test, preds, output_dict=True)\n",
        "    f1_row = {cls: report[cls][\"f1-score\"]\n",
        "              for cls in report.keys()\n",
        "              if cls not in [\"accuracy\", \"macro avg\", \"weighted avg\"]}\n",
        "    f1_row[\"model\"] = name\n",
        "    f1_results.append(f1_row)\n",
        "\n",
        "\n",
        "    # Save results\n",
        "    results[name] = acc\n",
        "\n",
        "    # Confusion matrix\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.heatmap(confusion_matrix(y_test, preds), annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "    plt.title(f\"Confusion Matrix - {name}\")\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"Actual\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAwjj8KV5WjZ"
      },
      "source": [
        "## DEEP LEARNING MODELS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYqGle6_JFJ0"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install tensorflow==2.12.0\n",
        "!pip install tensorflow-directml-plugin -f https://aka.ms/tensorflow-directml-plugin\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vSNLZxu5beb"
      },
      "outputs": [],
      "source": [
        "# CNN reshape\n",
        "X_train_np = X_train_scaled.to_numpy()\n",
        "X_test_np  = X_test_scaled.to_numpy()\n",
        "\n",
        "X_train_cnn = X_train_np.reshape(X_train_np.shape[0], X_train_np.shape[1], 1)\n",
        "X_test_cnn  = X_test_np.reshape(X_test_np.shape[0], X_test_np.shape[1], 1)\n",
        "\n",
        "# One-hot\n",
        "y_enc = le.fit_transform(y)\n",
        "num_classes = len(np.unique(y_enc))\n",
        "y_train_cat = tf.keras.utils.to_categorical(y_train_enc, num_classes)\n",
        "y_test_cat  = tf.keras.utils.to_categorical(y_test_enc, num_classes)\n",
        "\n",
        "# Building the 1D CNN\n",
        "def build_1d_cnn(input_shape, num_classes):\n",
        "    model = Sequential([\n",
        "        Conv1D(64, kernel_size=5, activation='relu', input_shape=input_shape),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling1D(pool_size=2),\n",
        "\n",
        "        Conv1D(128, kernel_size=5, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling1D(pool_size=2),\n",
        "\n",
        "        Conv1D(256, kernel_size=3, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling1D(pool_size=2),\n",
        "\n",
        "        Flatten(),\n",
        "        Dense(256, activation='relu'),\n",
        "        Dropout(0.4),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "input_shape = (X_train_cnn.shape[1], 1)\n",
        "cnn = build_1d_cnn(input_shape, num_classes)\n",
        "cnn.summary()\n",
        "\n",
        "es = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)\n",
        "rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
        "\n",
        "# Training\n",
        "history = cnn.fit(\n",
        "    X_train_cnn, y_train_cat,\n",
        "    epochs=60,\n",
        "    batch_size=32,\n",
        "    validation_split=0.15,\n",
        "    callbacks=[es, rlr],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "#Testing\n",
        "test_loss, test_acc = cnn.evaluate(X_test_cnn, y_test_cat, verbose=0)\n",
        "\n",
        "# Predictions\n",
        "preds_proba = cnn.predict(X_test_cnn)\n",
        "preds = np.argmax(preds_proba, axis=1)\n",
        "predicted_labels = le.inverse_transform(preds)\n",
        "true_labels = le.inverse_transform(y_test_enc)\n",
        "\n",
        "# Classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "#cnn_report=(classification_report(true_labels, predicted_labels))\n",
        "cnn_report = classification_report(true_labels, predicted_labels, output_dict=True)\n",
        "print(cnn_report)\n",
        "f1_row = {cls: cnn_report[cls][\"f1-score\"]\n",
        "          for cls in cnn_report.keys()\n",
        "          if cls not in [\"accuracy\", \"macro avg\", \"weighted avg\"]}\n",
        "f1_row[\"model\"] = \"1D CNN\"\n",
        "f1_results.append(f1_row)\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(true_labels, predicted_labels)\n",
        "plt.figure(figsize=(7,5))\n",
        "sns.heatmap(cm, annot=True, cmap=\"Greens\", fmt=\"d\")\n",
        "plt.title(\"Confusion Matrix - 1D CNN\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "InEu7Jn7DCxN"
      },
      "outputs": [],
      "source": [
        "final_f1_table = pd.DataFrame(f1_results).set_index(\"model\")\n",
        "display(final_f1_table)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sj5M3FyrjCvU"
      },
      "source": [
        "## FINAL MODEL SAVING\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M70aRr-BjHt4"
      },
      "outputs": [],
      "source": [
        "# saving encoder for xgboost\n",
        "joblib.dump(le, \"label_encoder.pkl\")\n",
        "# choosing xgboost as the final model as giving best results\n",
        "joblib.dump(models[\"XGBoost\"], \"xgboost_model.pkl\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "aY_DZ_SzSXQA",
        "pISY6wFJYA3k",
        "puAOa_TpdN1y",
        "craOO-_SiTkL",
        "xCyutLgqX1An",
        "CLsQAcLfY_SE",
        "eddta2IxZuKh",
        "3elbe1PmqnlZ",
        "JOc9xb7HIqBH",
        "CfcUgE9ZRp_-",
        "iAwjj8KV5WjZ"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
