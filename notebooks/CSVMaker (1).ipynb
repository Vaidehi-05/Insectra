{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d676b049-04bb-4753-a125-676d9461c6ec",
      "metadata": {
        "scrolled": true,
        "id": "d676b049-04bb-4753-a125-676d9461c6ec"
      },
      "outputs": [],
      "source": [
        "# ================================================================\n",
        "# ADVANCED AUDIO FEATURE EXTRACTION + MULTI-FOLDER MERGE + RESUME\n",
        "# ================================================================\n",
        "import random\n",
        "import os, tempfile, time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import librosa, noisereduce as nr\n",
        "from pydub import AudioSegment\n",
        "from pydub.utils import which\n",
        "from scipy.stats import entropy\n",
        "from scipy.signal import find_peaks\n",
        "\n",
        "# --- Verify FFmpeg ---\n",
        "ffmpeg_path, ffprobe_path = which(\"ffmpeg\"), which(\"ffprobe\")\n",
        "if ffmpeg_path is None or ffprobe_path is None:\n",
        "    raise EnvironmentError(\"‚ùå FFmpeg/ffprobe not found. Add them to PATH.\")\n",
        "else:\n",
        "    print(f\"‚úÖ FFmpeg: {ffmpeg_path}\")\n",
        "\n",
        "# --- numpy fix for librosa ---\n",
        "if not hasattr(np, 'complex'):\n",
        "    np.complex = np.complex128\n",
        "\n",
        "# ================================================================\n",
        "# PATHS (EDIT THIS PART)\n",
        "# ================================================================\n",
        "OUTPUT_CSV = r\"projectFile.csv\"\n",
        "\n",
        "# ü™∂ Add all folders you want to include below:\n",
        "AUDIO_FOLDERS = [\n",
        "    r\"E:\\Insect459\\Train\\Train\",\n",
        "]\n",
        "\n",
        "# ================================================================\n",
        "# AUDIO LOADING + CONVERSION\n",
        "# ================================================================\n",
        "def load_audio_any_format(file_path, target_sr=22050):\n",
        "    ext = os.path.splitext(file_path)[1].lower()\n",
        "    if ext != \".wav\":\n",
        "        with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as tmp:\n",
        "            AudioSegment.from_file(file_path).export(tmp.name, format=\"wav\")\n",
        "            tmp_path = tmp.name\n",
        "        file_path = tmp_path\n",
        "    y, sr = librosa.load(file_path, sr=target_sr, mono=True)\n",
        "    if \"tmp\" in file_path and os.path.exists(file_path):\n",
        "        try:\n",
        "            os.remove(file_path)\n",
        "        except OSError:\n",
        "            pass\n",
        "    return y, sr\n",
        "\n",
        "# ================================================================\n",
        "# AUDIO CLEANING\n",
        "# ================================================================\n",
        "def clean_audio(y, sr, segment_duration=3.0, num_segments=5):\n",
        "    \"\"\"Return multiple random segments of fixed duration from long audio.\"\"\"\n",
        "\n",
        "    # Trim silence\n",
        "    y, _ = librosa.effects.trim(y, top_db=20)\n",
        "    if len(y) < 0.2 * sr:\n",
        "        raise ValueError(\"Too short after trimming\")\n",
        "\n",
        "    # Denoise\n",
        "    y = nr.reduce_noise(y=y, sr=sr)\n",
        "\n",
        "    # Normalize\n",
        "    y = y / (np.max(np.abs(y)) + 1e-6)\n",
        "\n",
        "    segment_len = int(sr * segment_duration)\n",
        "\n",
        "    # If audio is shorter than one segment ‚Üí pad to segment size\n",
        "    if len(y) <= segment_len:\n",
        "        y = np.pad(y, (0, segment_len - len(y)))\n",
        "        return [y]  # one segment only\n",
        "\n",
        "    # If audio is long ‚Üí sample multiple segments\n",
        "    max_start = len(y) - segment_len\n",
        "    segments = []\n",
        "\n",
        "    for _ in range(num_segments):\n",
        "        start = random.randint(0, max_start)\n",
        "        seg = y[start:start + segment_len]\n",
        "        segments.append(seg)\n",
        "\n",
        "    return segments\n",
        "\n",
        "# ================================================================\n",
        "# FEATURE EXTRACTION (FULL SET)\n",
        "# ================================================================\n",
        "def extract_features(y, sr):\n",
        "    # Core spectral features\n",
        "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40, n_fft=2048, hop_length=512, fmin=50, fmax=12000)\n",
        "    delta, delta2 = librosa.feature.delta(mfcc), librosa.feature.delta(mfcc, order=2)\n",
        "    centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
        "    bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
        "    contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
        "    rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
        "    flatness = librosa.feature.spectral_flatness(y=y)\n",
        "    zcr = librosa.feature.zero_crossing_rate(y)\n",
        "    rms = librosa.feature.rms(y=y)\n",
        "\n",
        "    # Advanced metrics\n",
        "    S = np.abs(librosa.stft(y))\n",
        "    power_spec = S ** 2\n",
        "    spec_entropy = entropy(np.mean(power_spec, axis=1))\n",
        "    crest_factor = np.max(np.abs(y)) / (np.sqrt(np.mean(y**2)) + 1e-8)\n",
        "\n",
        "    # Band energy ratios\n",
        "    fmax = min(12000, sr // 2)\n",
        "    mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, fmax=fmax)\n",
        "    mel_db = librosa.power_to_db(mel_spec)\n",
        "    bands = [(200,1000), (1000,3000), (3000,8000), (8000,12000)]\n",
        "    mel_freqs = librosa.mel_frequencies(n_mels=128, fmin=0, fmax=fmax)\n",
        "    band_means = []\n",
        "    for low, high in bands:\n",
        "        mask = (mel_freqs >= low) & (mel_freqs < high)\n",
        "        band_means.append(np.mean(mel_db[mask]))\n",
        "    ratios = [band_means[i]/(np.sum(band_means)+1e-6) for i in range(len(band_means))]\n",
        "\n",
        "    # Peak frequencies\n",
        "    freqs = np.fft.rfftfreq(len(y), 1/sr)\n",
        "    psd = np.abs(np.fft.rfft(y))**2\n",
        "    peaks, _ = find_peaks(psd)\n",
        "    top_peaks = sorted(zip(psd[peaks], freqs[peaks]), reverse=True)[:3]\n",
        "    peaks_vals = [p[1] for p in top_peaks] + [0]*(3-len(top_peaks))\n",
        "\n",
        "    # Onset rate\n",
        "    onset_env = librosa.onset.onset_strength(y=y, sr=sr)\n",
        "    onset_rate = librosa.beat.tempo(onset_envelope=onset_env, sr=sr)[0]\n",
        "\n",
        "    # SNR estimate\n",
        "    signal_power = np.mean(y**2)\n",
        "    noise_floor = np.mean((y - librosa.effects.preemphasis(y))**2)\n",
        "    snr = 10 * np.log10((signal_power + 1e-6) / (noise_floor + 1e-6))\n",
        "\n",
        "    features = np.hstack([\n",
        "        np.mean(mfcc, axis=1), np.std(mfcc, axis=1),\n",
        "        np.mean(delta, axis=1), np.std(delta, axis=1),\n",
        "        np.mean(delta2, axis=1), np.std(delta2, axis=1),\n",
        "        np.mean(centroid), np.std(centroid),\n",
        "        np.mean(bandwidth), np.std(bandwidth),\n",
        "        np.mean(contrast), np.std(contrast),\n",
        "        np.mean(rolloff), np.std(rolloff),\n",
        "        np.mean(flatness), np.std(flatness),\n",
        "        np.mean(zcr), np.std(zcr),\n",
        "        np.mean(rms), np.std(rms),\n",
        "        spec_entropy, crest_factor,\n",
        "        *ratios, *peaks_vals,\n",
        "        onset_rate, snr\n",
        "    ])\n",
        "    return features\n",
        "\n",
        "# ================================================================\n",
        "# PROCESS ONE FOLDER (USED INTERNALLY)\n",
        "# ================================================================\n",
        "def process_folder(folder_path, processed_files, output_csv):\n",
        "    data = []\n",
        "    print(f\"\\nüìÇ Processing folder: {folder_path}\")\n",
        "\n",
        "    for root, _, files in os.walk(folder_path):\n",
        "        for file in files:\n",
        "            if not file.lower().endswith((\".wav\", \".mp3\", \".flac\")):\n",
        "                continue\n",
        "\n",
        "            # Skip duplicates based on filename\n",
        "            if file in processed_files:\n",
        "                continue\n",
        "\n",
        "            full_path = os.path.join(root, file)\n",
        "            label = os.path.basename(root)\n",
        "\n",
        "            try:\n",
        "                y, sr = load_audio_any_format(full_path)\n",
        "\n",
        "                # ‚¨áÔ∏è NEW ‚Üí multiple 3-second segments\n",
        "                segments = clean_audio(y, sr)\n",
        "\n",
        "                segment_index = 1\n",
        "                for seg in segments:\n",
        "                    feats = extract_features(seg, sr)\n",
        "                    # Save segment as new unique row\n",
        "                    new_filename = f\"{file}_seg{segment_index}\"\n",
        "                    data.append(np.append(feats, [label, new_filename]))\n",
        "                    segment_index += 1\n",
        "\n",
        "                print(f\"‚úÖ {file} ‚Üí {label}  ({len(segments)} segments)\")\n",
        "\n",
        "                # Save every 10 rows\n",
        "                if len(data) >= 10:\n",
        "                    safe_append_to_csv(data, output_csv)\n",
        "                    data.clear()\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Skipped {file}: {e}\")\n",
        "\n",
        "    if data:\n",
        "        safe_append_to_csv(data, output_csv)\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# SAFE CSV APPEND (auto create + retry)\n",
        "# ================================================================\n",
        "def safe_append_to_csv(data, output_csv, max_retries=5):\n",
        "    n_mfcc = 40\n",
        "    columns = (\n",
        "        [f\"mfcc_mean_{i}\" for i in range(n_mfcc)] +\n",
        "        [f\"mfcc_std_{i}\" for i in range(n_mfcc)] +\n",
        "        [f\"delta_mean_{i}\" for i in range(n_mfcc)] +\n",
        "        [f\"delta_std_{i}\" for i in range(n_mfcc)] +\n",
        "        [f\"delta2_mean_{i}\" for i in range(n_mfcc)] +\n",
        "        [f\"delta2_std_{i}\" for i in range(n_mfcc)] +\n",
        "        [\n",
        "            \"spectral_centroid_mean\",\"spectral_centroid_std\",\n",
        "            \"spectral_bandwidth_mean\",\"spectral_bandwidth_std\",\n",
        "            \"spectral_contrast_mean\",\"spectral_contrast_std\",\n",
        "            \"rolloff_mean\",\"rolloff_std\",\n",
        "            \"flatness_mean\",\"flatness_std\",\n",
        "            \"zcr_mean\",\"zcr_std\",\n",
        "            \"rms_mean\",\"rms_std\",\n",
        "            \"spectral_entropy\",\"crest_factor\",\n",
        "            \"band_low_ratio\",\"band_midlow_ratio\",\"band_midhigh_ratio\",\"band_high_ratio\",\n",
        "            \"peak_freq_1\",\"peak_freq_2\",\"peak_freq_3\",\n",
        "            \"onset_rate\",\"snr\",\"label\",\"file\"\n",
        "        ]\n",
        "    )\n",
        "    df = pd.DataFrame(data, columns=columns)\n",
        "    write_header = not os.path.exists(output_csv)\n",
        "    if write_header:\n",
        "        os.makedirs(os.path.dirname(output_csv), exist_ok=True)\n",
        "        print(f\"üÜï Creating new CSV file at: {output_csv}\")\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            df.to_csv(output_csv, mode='a', index=False, header=write_header)\n",
        "            return\n",
        "        except PermissionError:\n",
        "            print(f\"‚ö†Ô∏è File locked. Retry {attempt+1}/{max_retries} in 3s...\")\n",
        "            time.sleep(3)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Error writing to CSV: {e}\")\n",
        "            break\n",
        "    print(\"‚ùå Failed to append after multiple retries.\")\n",
        "\n",
        "# ================================================================\n",
        "# MAIN EXECUTION\n",
        "# ================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    processed_files = set()\n",
        "    if os.path.exists(OUTPUT_CSV):\n",
        "        df = pd.read_csv(OUTPUT_CSV)\n",
        "        if 'file' in df.columns:\n",
        "            processed_files = set(df['file'].values)\n",
        "        print(f\"üîÑ Resuming from existing CSV ({len(processed_files)} files found).\")\n",
        "    else:\n",
        "        print(f\"üÜï No CSV found, will create a new one at:\\n   {OUTPUT_CSV}\")\n",
        "\n",
        "    # Process all folders in the list\n",
        "    for folder in AUDIO_FOLDERS:\n",
        "        if os.path.exists(folder):\n",
        "            process_folder(folder, processed_files, OUTPUT_CSV)\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è Skipped (folder not found): {folder}\")\n",
        "\n",
        "    print(\"\\nüéâ All folders processed successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a116672-ec8f-468d-929d-042f12ab4eef",
      "metadata": {
        "id": "9a116672-ec8f-468d-929d-042f12ab4eef"
      },
      "outputs": [],
      "source": [
        "# ================================================================\n",
        "# MEMORY-SAFE FEATURE EXTRACTOR (FULL-SCHEMA) ‚Äî Segment-by-segment\n",
        "# Keeps SAME columns as your previous CSV; safe resume + low RAM\n",
        "# ================================================================\n",
        "import os, tempfile, time, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import librosa, noisereduce as nr\n",
        "from pydub import AudioSegment\n",
        "from pydub.utils import which\n",
        "from scipy.stats import entropy\n",
        "from scipy.signal import find_peaks\n",
        "\n",
        "# ----------------- CONFIG (edit) -----------------\n",
        "OUTPUT_CSV = r\"D:\\Projects\\Minor\\Audio\\InsectsXeno\\Audio\\insect459seg_audio_features.csv\"\n",
        "AUDIO_FOLDERS = [ r\"E:\\Insect459\\Train\\Train\" ]   # list of root folders to scan\n",
        "SEGMENT_DURATION = 3.0    # seconds per segment\n",
        "NUM_SEGMENTS = 5          # number of segments per file (set 1..5). Use 1 for minimal load.\n",
        "DENR_NOISE_FOR_ALL = False  # set True to apply denoising for all folders (default False)\n",
        "SKIP_FOLDERS = {\"other_insects\"}  # folder names to skip entirely\n",
        "# -------------------------------------------------\n",
        "\n",
        "# check ffmpeg\n",
        "ffmpeg_path, ffprobe_path = which(\"ffmpeg\"), which(\"ffprobe\")\n",
        "print(\"FFmpeg:\", ffmpeg_path)\n",
        "\n",
        "# numpy compatibility\n",
        "if not hasattr(np, \"complex\"):\n",
        "    np.complex = np.complex128\n",
        "\n",
        "# --- helper: detect esc-like folder (no denoise) ---\n",
        "def is_esc_folder(path):\n",
        "    return (\"esc\" in path.lower() or \"esc-50\" in path.lower())\n",
        "\n",
        "# ---------------- audio load ----------------\n",
        "def load_audio_any_format(file_path, target_sr=22050):\n",
        "    ext = os.path.splitext(file_path)[1].lower()\n",
        "    tmp_path = None\n",
        "    if ext != \".wav\":\n",
        "        tf = tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False)\n",
        "        tmp_path = tf.name\n",
        "        tf.close()\n",
        "        AudioSegment.from_file(file_path).export(tmp_path, format=\"wav\")\n",
        "        file_path = tmp_path\n",
        "    y, sr = librosa.load(file_path, sr=target_sr, mono=True)\n",
        "    if tmp_path and os.path.exists(tmp_path):\n",
        "        try:\n",
        "            os.remove(tmp_path)\n",
        "        except Exception:\n",
        "            pass\n",
        "    return y, sr\n",
        "\n",
        "# ---------------- get random segment(s) ----------------\n",
        "def get_random_segments(y, sr, duration=3.0, num_segments=5):\n",
        "    seg_len = int(sr * duration)\n",
        "    if len(y) <= seg_len:\n",
        "        # pad and return single segment\n",
        "        return [np.pad(y, (0, seg_len - len(y)))]\n",
        "    segments = []\n",
        "    max_start = len(y) - seg_len\n",
        "    # sample distinct start positions if possible\n",
        "    starts = set()\n",
        "    tries = 0\n",
        "    while len(starts) < num_segments and tries < num_segments*5:\n",
        "        starts.add(random.randint(0, max_start))\n",
        "        tries += 1\n",
        "    for s in sorted(list(starts))[:num_segments]:\n",
        "        segments.append(y[s:s+seg_len])\n",
        "    return segments\n",
        "\n",
        "# ---------------- feature extractor (same schema) ----------------\n",
        "def extract_features(y, sr):\n",
        "    # MFCCs + deltas\n",
        "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40, n_fft=2048, hop_length=512, fmin=50, fmax=min(12000, sr//2))\n",
        "    delta  = librosa.feature.delta(mfcc)\n",
        "    delta2 = librosa.feature.delta(mfcc, order=2)\n",
        "\n",
        "    # spectral features\n",
        "    centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
        "    bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
        "    contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
        "    rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
        "    flatness = librosa.feature.spectral_flatness(y=y)\n",
        "    zcr = librosa.feature.zero_crossing_rate(y)\n",
        "    rms = librosa.feature.rms(y=y)\n",
        "\n",
        "    # advanced metrics\n",
        "    S = np.abs(librosa.stft(y))\n",
        "    power_spec = S ** 2\n",
        "    spec_entropy = float(entropy(np.mean(power_spec, axis=1)))\n",
        "    crest_factor = float(np.max(np.abs(y)) / (np.sqrt(np.mean(y**2)) + 1e-8))\n",
        "\n",
        "    # band energy ratios (4 bands)\n",
        "    fmax = min(12000, sr // 2)\n",
        "    mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, fmax=fmax)\n",
        "    mel_db = librosa.power_to_db(mel_spec)\n",
        "    mel_freqs = librosa.mel_frequencies(n_mels=128, fmin=0, fmax=fmax)\n",
        "    bands = [(200,1000), (1000,3000), (3000,8000), (8000,12000)]\n",
        "    band_means = []\n",
        "    for low, high in bands:\n",
        "        mask = (mel_freqs >= low) & (mel_freqs < high)\n",
        "        # guard empty mask (rare)\n",
        "        if np.any(mask):\n",
        "            band_means.append(np.mean(mel_db[mask]))\n",
        "        else:\n",
        "            band_means.append(0.0)\n",
        "    total = (np.sum(band_means) + 1e-9)\n",
        "    ratios = [bm/total for bm in band_means]\n",
        "\n",
        "    # peak frequencies from PSD\n",
        "    freqs = np.fft.rfftfreq(len(y), 1/sr)\n",
        "    psd = np.abs(np.fft.rfft(y))**2\n",
        "    peaks, _ = find_peaks(psd)\n",
        "    top_peaks = sorted(zip(psd[peaks], freqs[peaks]), reverse=True)[:3] if len(peaks)>0 else []\n",
        "    peaks_vals = [p[1] for p in top_peaks] + [0.0]*(3-len(top_peaks))\n",
        "\n",
        "    # onset rate (librosa-version-safe)\n",
        "    onset_env = librosa.onset.onset_strength(y=y, sr=sr)\n",
        "    onset_frames = librosa.onset.onset_detect(onset_envelope=onset_env, sr=sr)\n",
        "    if len(onset_frames) > 1:\n",
        "        onset_times = librosa.frames_to_time(onset_frames, sr=sr)\n",
        "        onset_rate = float(1.0 / np.mean(np.diff(onset_times)))\n",
        "    else:\n",
        "        onset_rate = 0.0\n",
        "\n",
        "    # SNR estimate\n",
        "    signal_power = np.mean(y**2)\n",
        "    noise_floor = np.mean((y - librosa.effects.preemphasis(y))**2)\n",
        "    snr = float(10 * np.log10((signal_power + 1e-9) / (noise_floor + 1e-9)))\n",
        "\n",
        "    # flatten in the exact order used previously\n",
        "    features = np.hstack([\n",
        "        np.mean(mfcc, axis=1), np.std(mfcc, axis=1),\n",
        "        np.mean(delta, axis=1), np.std(delta, axis=1),\n",
        "        np.mean(delta2, axis=1), np.std(delta2, axis=1),\n",
        "        np.mean(centroid), np.std(centroid),\n",
        "        np.mean(bandwidth), np.std(bandwidth),\n",
        "        np.mean(contrast), np.std(contrast),\n",
        "        np.mean(rolloff), np.std(rolloff),\n",
        "        np.mean(flatness), np.std(flatness),\n",
        "        np.mean(zcr), np.std(zcr),\n",
        "        np.mean(rms), np.std(rms),\n",
        "        spec_entropy, crest_factor,\n",
        "        *ratios, *peaks_vals,\n",
        "        onset_rate, snr\n",
        "    ]).astype(float)\n",
        "\n",
        "    return features\n",
        "\n",
        "# ---------------- build columns (exact schema) ----------------\n",
        "def build_columns(n_mfcc=40):\n",
        "    cols = (\n",
        "        [f\"mfcc_mean_{i}\" for i in range(n_mfcc)] +\n",
        "        [f\"mfcc_std_{i}\" for i in range(n_mfcc)] +\n",
        "        [f\"delta_mean_{i}\" for i in range(n_mfcc)] +\n",
        "        [f\"delta_std_{i}\" for i in range(n_mfcc)] +\n",
        "        [f\"delta2_mean_{i}\" for i in range(n_mfcc)] +\n",
        "        [f\"delta2_std_{i}\" for i in range(n_mfcc)] +\n",
        "        [\n",
        "            \"spectral_centroid_mean\",\"spectral_centroid_std\",\n",
        "            \"spectral_bandwidth_mean\",\"spectral_bandwidth_std\",\n",
        "            \"spectral_contrast_mean\",\"spectral_contrast_std\",\n",
        "            \"rolloff_mean\",\"rolloff_std\",\n",
        "            \"flatness_mean\",\"flatness_std\",\n",
        "            \"zcr_mean\",\"zcr_std\",\n",
        "            \"rms_mean\",\"rms_std\",\n",
        "            \"spectral_entropy\",\"crest_factor\",\n",
        "            \"band_low_ratio\",\"band_midlow_ratio\",\"band_midhigh_ratio\",\"band_high_ratio\",\n",
        "            \"peak_freq_1\",\"peak_freq_2\",\"peak_freq_3\",\n",
        "            \"onset_rate\",\"snr\",\"label\",\"file\"\n",
        "        ]\n",
        "    )\n",
        "    return cols\n",
        "\n",
        "# ---------------- append single row safely ----------------\n",
        "def append_row_to_csv(row_values, output_csv, retry=5):\n",
        "    cols = build_columns()\n",
        "    df = pd.DataFrame([row_values], columns=cols)\n",
        "    write_header = not os.path.exists(output_csv)\n",
        "    for attempt in range(retry):\n",
        "        try:\n",
        "            df.to_csv(output_csv, mode='a', index=False, header=write_header)\n",
        "            return True\n",
        "        except PermissionError:\n",
        "            time.sleep(1 + attempt)\n",
        "        except Exception as e:\n",
        "            print(\"CSV write error:\", e)\n",
        "            return False\n",
        "    print(\"Failed to write after retries.\")\n",
        "    return False\n",
        "\n",
        "# ---------------- main processing loop (memory-safe) ----------------\n",
        "def process_folder(folder_path, processed_files, output_csv, segment_duration, num_segments):\n",
        "    print(\"\\nProcessing folder:\", folder_path)\n",
        "    for root, _, files in os.walk(folder_path):\n",
        "        label = os.path.basename(root)\n",
        "        if label in SKIP_FOLDERS:\n",
        "            print(\"Skipping folder (by config):\", root)\n",
        "            continue\n",
        "\n",
        "        for file in files:\n",
        "            if not file.lower().endswith((\".wav\", \".mp3\", \".flac\", \".m4a\")):\n",
        "                continue\n",
        "\n",
        "            # build unique processed key per segment e.g filename_seg1\n",
        "            base_filename = file\n",
        "            # if any segment was already processed, we will skip only those segments\n",
        "            # we store processed 'file' entries in CSV exactly as 'origname_segX'\n",
        "            try:\n",
        "                if num_segments == 1:\n",
        "                    seg_keys = [f\"{base_filename}_seg1\"]\n",
        "                else:\n",
        "                    seg_keys = [f\"{base_filename}_seg{i}\" for i in range(1, num_segments+1)]\n",
        "            except Exception:\n",
        "                seg_keys = [f\"{base_filename}_seg1\"]\n",
        "\n",
        "            # if all segments already processed, skip\n",
        "            if all(k in processed_files for k in seg_keys):\n",
        "                continue\n",
        "\n",
        "            full_path = os.path.join(root, file)\n",
        "            apply_denoise = DENR_NOISE_FOR_ALL or (not is_esc_folder(full_path))\n",
        "\n",
        "            try:\n",
        "                y, sr = load_audio_any_format(full_path)\n",
        "            except Exception as e:\n",
        "                print(\"Failed to load:\", file, \"|\", e)\n",
        "                continue\n",
        "\n",
        "            segments = get_random_segments(y, sr, duration=segment_duration, num_segments=num_segments)\n",
        "\n",
        "            for idx, seg in enumerate(segments, start=1):\n",
        "                seg_key = f\"{base_filename}_seg{idx}\"\n",
        "                if seg_key in processed_files:\n",
        "                    continue\n",
        "\n",
        "                # optionally denoise the segment only (small memory)\n",
        "                if apply_denoise:\n",
        "                    try:\n",
        "                        seg = nr.reduce_noise(y=seg, sr=sr)\n",
        "                    except Exception:\n",
        "                        # fallback: skip denoising if it fails\n",
        "                        pass\n",
        "\n",
        "                # normalize\n",
        "                if np.max(np.abs(seg)) > 0:\n",
        "                    seg = seg / (np.max(np.abs(seg)) + 1e-9)\n",
        "\n",
        "                try:\n",
        "                    feats = extract_features(seg, sr)\n",
        "                except Exception as e:\n",
        "                    print(\"Feature extraction failed for\", seg_key, \"|\", e)\n",
        "                    continue\n",
        "\n",
        "                row = list(feats) + [label, seg_key]\n",
        "\n",
        "                ok = append_row_to_csv(row, output_csv)\n",
        "                if ok:\n",
        "                    processed_files.add(seg_key)\n",
        "                    print(\"Saved:\", seg_key, \"->\", label)\n",
        "                else:\n",
        "                    print(\"Failed to save:\", seg_key)\n",
        "\n",
        "# ---------------- ENTRY POINT ----------------\n",
        "if __name__ == \"__main__\":\n",
        "    # load processed keys for resume\n",
        "    processed_files = set()\n",
        "    if os.path.exists(OUTPUT_CSV):\n",
        "        try:\n",
        "            df_done = pd.read_csv(OUTPUT_CSV)\n",
        "            if \"file\" in df_done.columns:\n",
        "                processed_files = set(df_done[\"file\"].astype(str).tolist())\n",
        "            print(\"Resuming. Already processed rows:\", len(processed_files))\n",
        "        except Exception as e:\n",
        "            print(\"Could not read existing CSV for resume:\", e)\n",
        "\n",
        "    for folder in AUDIO_FOLDERS:\n",
        "        if os.path.exists(folder):\n",
        "            process_folder(folder, processed_files, OUTPUT_CSV, SEGMENT_DURATION, NUM_SEGMENTS)\n",
        "        else:\n",
        "            print(\"Folder not found:\", folder)\n",
        "\n",
        "    print(\"\\nALL DONE.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "520c049c-3efb-4069-b32e-52e7c12088f0",
      "metadata": {
        "id": "520c049c-3efb-4069-b32e-52e7c12088f0"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(OUTPUT_CSV)\n",
        "print(\"Total rows:\", len(df))\n",
        "print(\"Unique file names:\", df['file'].nunique())\n",
        "print(\"Duplicates:\", len(df) - df['file'].nunique())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df5ae547-746d-4131-b7c5-526818a493a1",
      "metadata": {
        "id": "df5ae547-746d-4131-b7c5-526818a493a1"
      },
      "outputs": [],
      "source": [
        "\n",
        "pip install --upgrade librosa\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8bd6fe3-fe5e-4a2c-80e6-9cdab82621cc",
      "metadata": {
        "id": "d8bd6fe3-fe5e-4a2c-80e6-9cdab82621cc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "CSV_PATH = r\"D:\\Projects\\Minor\\Audio\\InsectsXeno\\Audio\\xenoinsect_audio_features.csv\"   # <<< change this\n",
        "\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "print(\"Rows:\", len(df), \"Columns:\", len(df.columns))\n",
        "# print(\"Columns:\", df.columns.tolist())\n",
        ".0\n",
        "\n",
        "# 1. Missing values\n",
        "missing = df.isna().sum()\n",
        "print(\"\\nMissing values per column (showing >0):\")\n",
        "print(missing[missing > 0])\n",
        "\n",
        "# 2. Duplicate files\n",
        "if 'file' in df.columns:\n",
        "    dup = df['file'].duplicated().sum()\n",
        "    print(\"\\nDuplicate file entries:\", dup)\n",
        "\n",
        "# 3. Label counts\n",
        "import pandas as pd\n",
        "\n",
        "pd.set_option('display.max_rows', None)  # show all rows\n",
        "pd.set_option('display.max_columns', None)  # show all columns (if needed)\n",
        "pd.set_option('display.width', None)  # don‚Äôt wrap lines\n",
        "pd.set_option('display.max_colwidth', None)  # don‚Äôt cut long labels\n",
        "\n",
        "if 'label' in df.columns:\n",
        "    counts = df['label'].value_counts()\n",
        "    print(\"\\nLabel distribution:\")\n",
        "    print(counts)\n",
        "\n",
        "\n",
        "# 4. Numeric feature stats\n",
        "num = df.select_dtypes(include=[np.number])\n",
        "print(\"\\nNumeric feature count:\", num.shape[1])\n",
        "stats = num.describe().T[['count','mean','std','min','25%','50%','75%','max']]\n",
        "print(\"\\nFeature summary (first 10 rows):\")\n",
        "print(stats.head(10))\n",
        "\n",
        "# 5. Constant features\n",
        "const = [c for c in num.columns if num[c].nunique()<=1]\n",
        "print(\"\\nConstant features (should be none):\", const)\n",
        "\n",
        "# 6. NaN rate threshold\n",
        "nan_rate = (missing / len(df)).max()\n",
        "print(\"\\nMax NaN rate across columns:\", nan_rate)\n",
        "\n",
        "# Save simple report\n",
        "report_path = os.path.splitext(CSV_PATH)[0] + \"_diagnostic_report.csv\"\n",
        "stats.to_csv(report_path)\n",
        "print(\"\\nSaved numeric stats to:\", report_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2726c083-94b8-4e2e-845f-e2747749f5a0",
      "metadata": {
        "id": "2726c083-94b8-4e2e-845f-e2747749f5a0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd, numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "import seaborn as sns   # optional if available\n",
        "\n",
        "CSV_PATH = r\"D:\\Projects\\Minor\\Audio\\InsectsXeno\\Audio\\insect459seg_audio_features.csv\"\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "num = df.select_dtypes(include=[np.number]).fillna(0)\n",
        "\n",
        "# 1. Class balance bar chart\n",
        "if 'label' in df.columns:\n",
        "    vc = df['label'].value_counts()\n",
        "    plt.figure(figsize=(16,9)); vc.plot(kind='bar'); plt.title(\"Class counts\"); plt.show()\n",
        "\n",
        "# 2. Histogram of a sample feature (e.g., mfcc_mean_0)\n",
        "col = [c for c in num.columns if 'mfcc_mean_0' in c or True][0]\n",
        "plt.figure(figsize=(18,9)); plt.hist(num[col], bins=50); plt.title(col); plt.show()\n",
        "\n",
        "# 3. Correlation heatmap (top 50 features by variance to keep plot readable)\n",
        "var_sorted = num.var().sort_values(ascending=False)\n",
        "top = var_sorted.index[:50]\n",
        "plt.figure(figsize=(20,16))\n",
        "corr = num[top].corr()\n",
        "sns.heatmap(corr, cmap='coolwarm', vmin=-1, vmax=1)\n",
        "plt.title(\"Top-50 feature correlation\"); plt.show()\n",
        "\n",
        "# 4. PCA 2D colored by label\n",
        "labels = df['label'] if 'label' in df.columns else None\n",
        "pca = PCA(n_components=2)\n",
        "Xp = pca.fit_transform(num.sample(n=min(len(num),2000), random_state=0))  # sample for speed\n",
        "if labels is not None:\n",
        "    lab_sample = labels.sample(n=min(len(labels),2000), random_state=0).values\n",
        "else:\n",
        "    lab_sample = None\n",
        "\n",
        "plt.figure(figsize=(16,16))\n",
        "if lab_sample is not None:\n",
        "    for lab in np.unique(lab_sample):\n",
        "        mask = (lab_sample==lab)\n",
        "        plt.scatter(Xp[mask,0], Xp[mask,1], label=str(lab), s=10)\n",
        "    plt.legend(fontsize=6, loc='best')\n",
        "else:\n",
        "    plt.scatter(Xp[:,0], Xp[:,1], s=10)\n",
        "plt.title(\"PCA 2D\"); plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe1ba896-150b-46d9-b36e-583d681db2b9",
      "metadata": {
        "id": "fe1ba896-150b-46d9-b36e-583d681db2b9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# ================================\n",
        "# LOAD DATA\n",
        "# ================================\n",
        "CSV = r\"D:\\Projects\\Minor\\Audio\\InsectsXeno\\Audio\\insect459seg_audio_features.csv\"\n",
        "df = pd.read_csv(CSV)\n",
        "\n",
        "# Extract features and labels\n",
        "X = df.select_dtypes(include=[np.number]).values\n",
        "y = df['label'].values\n",
        "\n",
        "print(\"Total samples:\", X.shape[0])\n",
        "print(\"Total features:\", X.shape[1])\n",
        "print(\"Classes:\", len(np.unique(y)))\n",
        "\n",
        "# ================================\n",
        "# OPTIONAL: SAMPLE FOR SPEED\n",
        "# ================================\n",
        "MAX_SAMPLES = 25000    # <-- you can increase or decrease\n",
        "if X.shape[0] > MAX_SAMPLES:\n",
        "    idx = np.random.choice(X.shape[0], MAX_SAMPLES, replace=False)\n",
        "    X, y = X[idx], y[idx]\n",
        "    print(f\"‚ö†Ô∏è Dataset reduced to {MAX_SAMPLES} samples for faster training.\")\n",
        "\n",
        "# ================================\n",
        "# REMOVE CONSTANT FEATURES\n",
        "# ================================\n",
        "stds = X.std(axis=0)\n",
        "constant_cols = np.where(stds == 0)[0]\n",
        "\n",
        "if len(constant_cols) > 0:\n",
        "    print(\"Removing constant features:\", constant_cols)\n",
        "    X = np.delete(X, constant_cols, axis=1)\n",
        "else:\n",
        "    print(\"No constant features detected.\")\n",
        "\n",
        "# ================================\n",
        "# RANDOM FOREST MODEL\n",
        "# ================================\n",
        "clf = RandomForestClassifier(\n",
        "    n_estimators=300,\n",
        "    max_depth=None,\n",
        "    n_jobs=-1,\n",
        "    class_weight=\"balanced_subsample\",   # <-- IMPORTANT for imbalanced insects\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# ================================\n",
        "# 5-FOLD STRATIFIED CROSS-VALIDATION\n",
        "# ================================\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "scores = cross_val_score(\n",
        "    clf,\n",
        "    X,\n",
        "    y,\n",
        "    cv=cv,\n",
        "    scoring=\"accuracy\",\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "print(\"\\n========== CROSS-VALIDATION RESULTS ==========\")\n",
        "print(\"Fold scores:\", scores)\n",
        "print(\"Mean CV accuracy:\", scores.mean())\n",
        "\n",
        "# ================================\n",
        "# TRAIN/TEST SPLIT FOR DETAILED REPORT\n",
        "# ================================\n",
        "Xtr, Xte, ytr, yte = train_test_split(\n",
        "    X, y, test_size=0.20, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "clf.fit(Xtr, ytr)\n",
        "yp = clf.predict(Xte)\n",
        "\n",
        "print(\"\\n========== CLASSIFICATION REPORT (TEST SET) ==========\")\n",
        "print(classification_report(yte, yp, zero_division=0))\n",
        "\n",
        "# Confusion matrix (optional)\n",
        "cm = confusion_matrix(yte, yp)\n",
        "print(\"Confusion matrix shape:\", cm.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f57afcd4-f732-46c8-b9d6-33772f3953f0",
      "metadata": {
        "scrolled": true,
        "id": "f57afcd4-f732-46c8-b9d6-33772f3953f0"
      },
      "outputs": [],
      "source": [
        "# ================================================================\n",
        "# ADVANCED AUDIO FEATURE EXTRACTION + MULTI-FOLDER MERGE + RESUME\n",
        "# WITH ESC-50 COMPATIBILITY + ENVIRONMENT LABEL\n",
        "# ================================================================\n",
        "import random\n",
        "import os, tempfile, time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import librosa, noisereduce as nr\n",
        "from pydub import AudioSegment\n",
        "from pydub.utils import which\n",
        "from scipy.stats import entropy\n",
        "from scipy.signal import find_peaks\n",
        "\n",
        "# --- Verify FFmpeg ---\n",
        "ffmpeg_path, ffprobe_path = which(\"ffmpeg\"), which(\"ffprobe\")\n",
        "if ffmpeg_path is None or ffprobe_path is None:\n",
        "    raise EnvironmentError(\"‚ùå FFmpeg/ffprobe not found. Add them to PATH.\")\n",
        "else:\n",
        "    print(f\"‚úÖ FFmpeg: {ffmpeg_path}\")\n",
        "\n",
        "# --- numpy fix for librosa ---\n",
        "if not hasattr(np, 'complex'):\n",
        "    np.complex = np.complex128\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# PATHS ‚Äî EDIT THESE\n",
        "# ================================================================\n",
        "OUTPUT_CSV = r\"D:\\Projects\\Minor\\Audio\\InsectsXeno\\Audio\\env_audio_features.csv\"\n",
        "\n",
        "AUDIO_FOLDERS = [\n",
        "    r\"D:\\Projects\\Minor\\Audio\\InsectsXeno\\Audio\\ESC-50-master\\ESC-50-master\\audio\"   # üîß Add ESC-50 here\n",
        "]\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# HELPER ‚Äî DETECT IF FILE BELONGS TO ESC-50\n",
        "# ================================================================\n",
        "def is_esc50(path):\n",
        "    return (\"esc\" in path.lower() or \"esc-50\" in path.lower())\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# AUDIO LOADING + CONVERSION\n",
        "# ================================================================\n",
        "def load_audio_any_format(file_path, target_sr=22050):\n",
        "    ext = os.path.splitext(file_path)[1].lower()\n",
        "\n",
        "    if ext != \".wav\":\n",
        "        with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as tmp:\n",
        "            AudioSegment.from_file(file_path).export(tmp.name, format=\"wav\")\n",
        "            tmp_path = tmp.name\n",
        "        file_path = tmp_path\n",
        "\n",
        "    y, sr = librosa.load(file_path, sr=target_sr, mono=True)\n",
        "\n",
        "    if \"tmp\" in file_path and os.path.exists(file_path):\n",
        "        try:\n",
        "            os.remove(file_path)\n",
        "        except OSError:\n",
        "            pass\n",
        "\n",
        "    return y, sr\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# AUDIO CLEANING ‚Äî ESC50 gets no denoise (üîß MODIFIED)\n",
        "# ================================================================\n",
        "def clean_audio(y, sr, segment_duration=3.0, num_segments=5, apply_denoise=True):\n",
        "\n",
        "    # trim silence\n",
        "    y, _ = librosa.effects.trim(y, top_db=20)\n",
        "    if len(y) < 0.2 * sr:\n",
        "        raise ValueError(\"Too short after trimming\")\n",
        "\n",
        "    # üîß ESC-50 should NOT be denoised\n",
        "    if apply_denoise:\n",
        "        y = nr.reduce_noise(y=y, sr=sr)\n",
        "\n",
        "    # normalize\n",
        "    y = y / (np.max(np.abs(y)) + 1e-6)\n",
        "\n",
        "    segment_len = int(sr * segment_duration)\n",
        "\n",
        "    # if too short ‚Üí pad\n",
        "    if len(y) <= segment_len:\n",
        "        y = np.pad(y, (0, segment_len - len(y)))\n",
        "        return [y]\n",
        "\n",
        "    # random segments for long audio\n",
        "    max_start = len(y) - segment_len\n",
        "    segments = []\n",
        "\n",
        "    for _ in range(num_segments):\n",
        "        start = random.randint(0, max_start)\n",
        "        seg = y[start:start + segment_len]\n",
        "        segments.append(seg)\n",
        "\n",
        "    return segments\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# FEATURE EXTRACTION (unchanged)\n",
        "# ================================================================\n",
        "def extract_features(y, sr):\n",
        "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40, n_fft=2048, hop_length=512, fmin=50, fmax=12000)\n",
        "    delta = librosa.feature.delta(mfcc)\n",
        "    delta2 = librosa.feature.delta(mfcc, order=2)\n",
        "\n",
        "    centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
        "    bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
        "    contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
        "    rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
        "    flatness = librosa.feature.spectral_flatness(y=y)\n",
        "    zcr = librosa.feature.zero_crossing_rate(y)\n",
        "    rms = librosa.feature.rms(y=y)\n",
        "\n",
        "    S = np.abs(librosa.stft(y))\n",
        "    power_spec = S ** 2\n",
        "    spec_entropy = entropy(np.mean(power_spec, axis=1))\n",
        "    crest_factor = np.max(np.abs(y)) / (np.sqrt(np.mean(y**2)) + 1e-8)\n",
        "\n",
        "    # band energy\n",
        "    fmax = min(12000, sr // 2)\n",
        "    mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, fmax=fmax)\n",
        "    mel_db = librosa.power_to_db(mel_spec)\n",
        "    mel_freqs = librosa.mel_frequencies(n_mels=128, fmin=0, fmax=fmax)\n",
        "\n",
        "    bands = [(200,1000), (1000,3000), (3000,8000), (8000,12000)]\n",
        "    band_means = []\n",
        "\n",
        "    for low, high in bands:\n",
        "        mask = (mel_freqs >= low) & (mel_freqs < high)\n",
        "        band_means.append(np.mean(mel_db[mask]))\n",
        "\n",
        "    ratios = [band_means[i] / (np.sum(band_means) + 1e-6) for i in range(4)]\n",
        "\n",
        "    # peaks\n",
        "    freqs = np.fft.rfftfreq(len(y), 1/sr)\n",
        "    psd = np.abs(np.fft.rfft(y))**2\n",
        "    peaks, _ = find_peaks(psd)\n",
        "    top_peaks = sorted(zip(psd[peaks], freqs[peaks]), reverse=True)[:3]\n",
        "    peak_vals = [p[1] for p in top_peaks] + [0]*(3 - len(top_peaks))\n",
        "\n",
        "    # onset\n",
        "    onset_env = librosa.onset.onset_strength(y=y, sr=sr)\n",
        "    onset_rate = librosa.feature.rhythm.tempo(onset_envelope=onset_env, sr=sr)[0]\n",
        "\n",
        "    # snr\n",
        "    signal_power = np.mean(y**2)\n",
        "    noise_floor = np.mean((y - librosa.effects.preemphasis(y))**2)\n",
        "    snr = 10 * np.log10((signal_power + 1e-6) / (noise_floor + 1e-6))\n",
        "\n",
        "    features = np.hstack([\n",
        "        np.mean(mfcc, axis=1), np.std(mfcc, axis=1),\n",
        "        np.mean(delta, axis=1), np.std(delta, axis=1),\n",
        "        np.mean(delta2, axis=1), np.std(delta2, axis=1),\n",
        "        np.mean(centroid), np.std(centroid),\n",
        "        np.mean(bandwidth), np.std(bandwidth),\n",
        "        np.mean(contrast), np.std(contrast),\n",
        "        np.mean(rolloff), np.std(rolloff),\n",
        "        np.mean(flatness), np.std(flatness),\n",
        "        np.mean(zcr), np.std(zcr),\n",
        "        np.mean(rms), np.std(rms),\n",
        "        spec_entropy, crest_factor,\n",
        "        *ratios,\n",
        "        *peak_vals,\n",
        "        onset_rate, snr\n",
        "    ])\n",
        "    return features\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# PROCESS A FOLDER\n",
        "# ================================================================\n",
        "def process_folder(folder_path, processed_files, output_csv):\n",
        "    data = []\n",
        "\n",
        "    print(f\"\\nüìÇ Processing folder: {folder_path}\")\n",
        "\n",
        "    for root, _, files in os.walk(folder_path):\n",
        "        for file in files:\n",
        "\n",
        "            if not file.lower().endswith((\".wav\", \".mp3\", \".flac\")):\n",
        "                continue\n",
        "\n",
        "            if file in processed_files:\n",
        "                continue\n",
        "\n",
        "            full_path = os.path.join(root, file)\n",
        "\n",
        "            # üîß ESC-50 ‚Üí label = \"environment\"\n",
        "            if is_esc50(full_path):\n",
        "                label = \"environment\"\n",
        "                apply_denoise = False  # üîß DO NOT denoise ESC50\n",
        "            else:\n",
        "                label = os.path.basename(root)\n",
        "                apply_denoise = True\n",
        "\n",
        "            try:\n",
        "                y, sr = load_audio_any_format(full_path)\n",
        "\n",
        "                # get 3-second segments\n",
        "                segments = clean_audio(y, sr, apply_denoise=apply_denoise)\n",
        "\n",
        "                seg_idx = 1\n",
        "                for seg in segments:\n",
        "                    feats = extract_features(seg, sr)\n",
        "                    new_filename = f\"{file}_seg{seg_idx}\"\n",
        "                    data.append(np.append(feats, [label, new_filename]))\n",
        "                    seg_idx += 1\n",
        "\n",
        "                print(f\"‚úÖ {file} ‚Üí {label} ({len(segments)} segments)\")\n",
        "\n",
        "                if len(data) >= 10:\n",
        "                    safe_append_to_csv(data, output_csv)\n",
        "                    data.clear()\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Skipped {file}: {e}\")\n",
        "\n",
        "    if data:\n",
        "        safe_append_to_csv(data, output_csv)\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# CSV APPENDER\n",
        "# ================================================================\n",
        "def safe_append_to_csv(data, output_csv, max_retries=5):\n",
        "    n_mfcc = 40\n",
        "    columns = (\n",
        "        [f\"mfcc_mean_{i}\" for i in range(n_mfcc)] +\n",
        "        [f\"mfcc_std_{i}\" for i in range(n_mfcc)] +\n",
        "        [f\"delta_mean_{i}\" for i in range(n_mfcc)] +\n",
        "        [f\"delta_std_{i}\" for i in range(n_mfcc)] +\n",
        "        [f\"delta2_mean_{i}\" for i in range(n_mfcc)] +\n",
        "        [f\"delta2_std_{i}\" for i in range(n_mfcc)] +\n",
        "        [\n",
        "            \"spectral_centroid_mean\",\"spectral_centroid_std\",\n",
        "            \"spectral_bandwidth_mean\",\"spectral_bandwidth_std\",\n",
        "            \"spectral_contrast_mean\",\"spectral_contrast_std\",\n",
        "            \"rolloff_mean\",\"rolloff_std\",\n",
        "            \"flatness_mean\",\"flatness_std\",\n",
        "            \"zcr_mean\",\"zcr_std\",\n",
        "            \"rms_mean\",\"rms_std\",\n",
        "            \"spectral_entropy\",\"crest_factor\",\n",
        "            \"band_low_ratio\",\"band_midlow_ratio\",\"band_midhigh_ratio\",\"band_high_ratio\",\n",
        "            \"peak_freq_1\",\"peak_freq_2\",\"peak_freq_3\",\n",
        "            \"onset_rate\",\"snr\",\"label\",\"file\"\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    df = pd.DataFrame(data, columns=columns)\n",
        "    write_header = not os.path.exists(output_csv)\n",
        "\n",
        "    if write_header:\n",
        "        os.makedirs(os.path.dirname(output_csv), exist_ok=True)\n",
        "        print(f\"üÜï Creating CSV at: {output_csv}\")\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            df.to_csv(output_csv, mode='a', index=False, header=write_header)\n",
        "            return\n",
        "        except PermissionError:\n",
        "            print(f\"‚ö†Ô∏è CSV locked. Retry {attempt+1}/{max_retries}...\")\n",
        "            time.sleep(3)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå CSV write error: {e}\")\n",
        "            return\n",
        "\n",
        "\n",
        "# ================================================================\n",
        "# MAIN\n",
        "# ================================================================\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    processed_files = set()\n",
        "\n",
        "    if os.path.exists(OUTPUT_CSV):\n",
        "        df = pd.read_csv(OUTPUT_CSV)\n",
        "        if \"file\" in df.columns:\n",
        "            processed_files = set(df[\"file\"].values)\n",
        "            print(f\"üîÑ Resuming ‚Äî found {len(processed_files)} processed files.\")\n",
        "    else:\n",
        "        print(f\"üÜï No CSV found ‚Äî will create: {OUTPUT_CSV}\")\n",
        "\n",
        "    for folder in AUDIO_FOLDERS:\n",
        "        if os.path.exists(folder):\n",
        "            process_folder(folder, processed_files, OUTPUT_CSV)\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è Folder not found: {folder}\")\n",
        "\n",
        "    print(\"\\nüéâ ALL DONE! CSV READY.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1f5bb5f-5db3-44bc-90b8-2503429bf962",
      "metadata": {
        "scrolled": true,
        "id": "d1f5bb5f-5db3-44bc-90b8-2503429bf962"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# ===============================================================\n",
        "# 1. LOAD CSV\n",
        "# ===============================================================\n",
        "\n",
        "CSV_PATH = r\"D:\\Projects\\Minor\\Audio\\InsectsXeno\\Audio\\env_audio_features.csv\"\n",
        "\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "print(\"üü¢ CSV Loaded:\", CSV_PATH)\n",
        "print(\"Shape:\", df.shape)\n",
        "\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "# 2. BASIC STRUCTURE CHECKS\n",
        "# ===============================================================\n",
        "\n",
        "print(\"\\n===== COLUMN CHECK =====\")\n",
        "expected_feature_count = (\n",
        "    40*6 +  # mfcc_mean, mfcc_std, delta_mean, delta_std, delta2_mean, delta2_std\n",
        "    2*7 +   # spectral_* stats\n",
        "    4 +     # band ratios\n",
        "    3 +     # peak freqs\n",
        "    2       # onset_rate, snr\n",
        ")\n",
        "\n",
        "expected_total = expected_feature_count + 2  # + label + file\n",
        "\n",
        "print(f\"Columns: {len(df.columns)} (expected ~{expected_total})\")\n",
        "\n",
        "if len(df.columns) != expected_total:\n",
        "    print(\"‚ö†Ô∏è Column count mismatch!\")\n",
        "else:\n",
        "    print(\"‚úÖ Column count correct.\")\n",
        "\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "# 3. CHECK FOR MISSING VALUES\n",
        "# ===============================================================\n",
        "\n",
        "print(\"\\n===== MISSING VALUE CHECK =====\")\n",
        "missing = df.isnull().sum().sum()\n",
        "print(f\"Total missing values: {missing}\")\n",
        "\n",
        "if missing > 0:\n",
        "    print(df.isnull().sum()[df.isnull().sum() > 0])\n",
        "    print(\"‚ö†Ô∏è There are missing values!\")\n",
        "else:\n",
        "    print(\"‚úÖ No missing values.\")\n",
        "\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "# 4. CHECK FOR INFINITE VALUES\n",
        "# ===============================================================\n",
        "\n",
        "print(\"\\n===== INFINITY CHECK =====\")\n",
        "\n",
        "# Select only numeric columns\n",
        "numeric_df = df.select_dtypes(include=[np.number])\n",
        "\n",
        "if np.isinf(numeric_df.values).any():\n",
        "    print(\"‚ö†Ô∏è CSV contains infinite values!\")\n",
        "else:\n",
        "    print(\"‚úÖ No infinite values found.\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "# 5. CHECK LABEL DISTRIBUTION (Class Imbalance)\n",
        "# ===============================================================\n",
        "\n",
        "print(\"\\n===== LABEL DISTRIBUTION =====\")\n",
        "if \"label\" in df.columns:\n",
        "    print(df[\"label\"].value_counts())\n",
        "else:\n",
        "    print(\"‚ùå No 'label' column found!\")\n",
        "\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "# 6. CHECK DUPLICATE FILES + ROWS\n",
        "# ===============================================================\n",
        "\n",
        "print(\"\\n===== DUPLICATE CHECK =====\")\n",
        "\n",
        "if \"file\" in df.columns:\n",
        "    dup_files = df[\"file\"].duplicated().sum()\n",
        "    print(\"Duplicate files:\", dup_files)\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No 'file' column found.\")\n",
        "\n",
        "dup_rows = df.duplicated().sum()\n",
        "print(\"Duplicate rows:\", dup_rows)\n",
        "\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "# 7. FEATURE DISTRIBUTION SUMMARY (detect broken data)\n",
        "# ===============================================================\n",
        "\n",
        "print(\"\\n===== FEATURE STATISTICS =====\")\n",
        "stats = df.describe()\n",
        "print(stats)\n",
        "\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "# 8. CHECK FOR ZERO-VARIANCE COLUMNS (bad features)\n",
        "# ===============================================================\n",
        "\n",
        "print(\"\\n===== ZERO VARIANCE FEATURE CHECK =====\")\n",
        "zero_var = stats.loc[\"std\"] == 0\n",
        "if zero_var.any():\n",
        "    print(\"‚ö†Ô∏è Zero-variance columns found:\")\n",
        "    print(zero_var[zero_var == True])\n",
        "else:\n",
        "    print(\"‚úÖ No zero-variance columns.\")\n",
        "\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "# 9. CHECK MFCC RANGES (detect extraction bugs)\n",
        "# ===============================================================\n",
        "\n",
        "print(\"\\n===== MFCC RANGE CHECK =====\")\n",
        "mfcc_cols = [col for col in df.columns if \"mfcc_mean\" in col]\n",
        "\n",
        "if len(mfcc_cols) != 40:\n",
        "    print(\"‚ö†Ô∏è Expected 40 MFCC mean features, found:\", len(mfcc_cols))\n",
        "else:\n",
        "    mean_range = df[mfcc_cols].describe()\n",
        "    print(\"MFCC mean min/max:\")\n",
        "    print(\"Min:\", mean_range.loc[\"min\"].min())\n",
        "    print(\"Max:\", mean_range.loc[\"max\"].max())\n",
        "    print(\"Range looks normal.\")\n",
        "\n",
        "\n",
        "# Count how many columns are NOT part of expected schema\n",
        "expected_cols = set([\n",
        "    *[f\"mfcc_mean_{i}\" for i in range(40)],\n",
        "    *[f\"mfcc_std_{i}\" for i in range(40)],\n",
        "    *[f\"delta_mean_{i}\" for i in range(40)],\n",
        "    *[f\"delta_std_{i}\" for i in range(40)],\n",
        "    *[f\"delta2_mean_{i}\" for i in range(40)],\n",
        "    *[f\"delta2_std_{i}\" for i in range(40)],\n",
        "    \"spectral_centroid_mean\",\"spectral_centroid_std\",\n",
        "    \"spectral_bandwidth_mean\",\"spectral_bandwidth_std\",\n",
        "    \"spectral_contrast_mean\",\"spectral_contrast_std\",\n",
        "    \"rolloff_mean\",\"rolloff_std\",\n",
        "    \"flatness_mean\",\"flatness_std\",\n",
        "    \"zcr_mean\",\"zcr_std\",\n",
        "    \"rms_mean\",\"rms_std\",\n",
        "    \"spectral_entropy\",\"crest_factor\",\n",
        "    \"band_low_ratio\",\"band_midlow_ratio\",\"band_midhigh_ratio\",\"band_high_ratio\",\n",
        "    \"peak_freq_1\",\"peak_freq_2\",\"peak_freq_3\",\n",
        "    \"onset_rate\",\"snr\",\n",
        "    \"label\",\"file\"\n",
        "])\n",
        "\n",
        "actual_cols = set(df.columns)\n",
        "\n",
        "extra = actual_cols - expected_cols\n",
        "missing = expected_cols - actual_cols\n",
        "\n",
        "print(\"EXTRA COLUMNS:\", extra)\n",
        "print(\"MISSING COLUMNS:\", missing)\n",
        "\n",
        "\n",
        "# ===============================================================\n",
        "# 10. PRINT CONCLUSION\n",
        "# ===============================================================\n",
        "\n",
        "print(\"\\n===== FINAL VALIDATION =====\")\n",
        "if missing == 0 and dup_rows == 0:\n",
        "    print(\"üéâ CSV looks clean and ready for ML!\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è CSV contains issues. Review warnings above.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c898d60-466c-46c5-8dce-6c09bbeed052",
      "metadata": {
        "id": "1c898d60-466c-46c5-8dce-6c09bbeed052"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Clean ESC-50, relabel insect datasets (TOP3 + Others), and merge all datasets.\n",
        "Fully corrected version.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# -------------------------\n",
        "# CONFIG ‚Äî EDIT THESE PATHS\n",
        "# -------------------------\n",
        "INSECT459_CSV = r\"D:\\Projects\\Minor\\Audio\\InsectsXeno\\Audio\\insect459seg_audio_features.csv\"\n",
        "XENO_CSV      = r\"D:\\Projects\\Minor\\Audio\\InsectsXeno\\Audio\\xenoinsect_audio_features.csv\"\n",
        "ESC50_META    = r\"D:\\Projects\\Minor\\Audio\\InsectsXeno\\Audio\\esc50.csv\"\n",
        "ESC50_FEATURES_CSV = r\"D:\\Projects\\Minor\\Audio\\InsectsXeno\\Audio\\env_audio_features.csv\"\n",
        "\n",
        "OUTPUT_DIR = r\"D:\\Projects\\Minor\\Audio\\InsectsXeno\\Audio\\merged_output\"\n",
        "\n",
        "# Your chosen top 3\n",
        "TOP3 = [\n",
        "    \"Chorthippus_biguttulus\",\n",
        "    \"Gryllus_bimaculatus\",\n",
        "    \"Ruspolia_nitidula\"\n",
        "]\n",
        "\n",
        "# ESC insect-related keyword filtering\n",
        "ESC_INSECT_KEYWORDS = {\n",
        "    \"insect\", \"insects\", \"cricket\", \"crickets\", \"grasshopper\",\n",
        "    \"bee\", \"mosquito\", \"fly\", \"flies\", \"buzz\", \"cicada\"\n",
        "}\n",
        "\n",
        "# -------------------------\n",
        "def ensure_dir(p):\n",
        "    Path(p).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def safe_read_csv(path):\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(path)\n",
        "    return pd.read_csv(path)\n",
        "\n",
        "# -------------------------\n",
        "# LOAD FILES\n",
        "# -------------------------\n",
        "ensure_dir(OUTPUT_DIR)\n",
        "print(\"Loading datasets...\")\n",
        "\n",
        "df_insect459 = safe_read_csv(INSECT459_CSV)\n",
        "df_xeno = safe_read_csv(XENO_CSV)\n",
        "df_esc_meta = safe_read_csv(ESC50_META)\n",
        "df_esc_feats = safe_read_csv(ESC50_FEATURES_CSV)\n",
        "\n",
        "print(\"Loaded shapes:\")\n",
        "print(\"  insect459:\", df_insect459.shape)\n",
        "print(\"  xenocanto:\", df_xeno.shape)\n",
        "print(\"  ESC meta:\", df_esc_meta.shape)\n",
        "print(\"  ESC feats:\", df_esc_feats.shape)\n",
        "\n",
        "# -------------------------\n",
        "# CLEAN ESC METADATA\n",
        "# -------------------------\n",
        "print(\"\\nCleaning ESC-50...\")\n",
        "\n",
        "# Normalize metadata filename column\n",
        "meta_fname_col = \"filename\"\n",
        "df_esc_meta[\"file_clean\"] = df_esc_meta[meta_fname_col].str.lower()\n",
        "\n",
        "# Normalize ESC feature filenames: remove _segX\n",
        "df_esc_feats[\"file_clean\"] = (\n",
        "    df_esc_feats[\"file\"].str.lower()\n",
        "    .str.replace(r\"_seg\\d+$\", \"\", regex=True)\n",
        ")\n",
        "\n",
        "# Merge metadata into feature rows\n",
        "esc_merged = df_esc_feats.merge(\n",
        "    df_esc_meta[[\"file_clean\", \"category\"]],\n",
        "    on=\"file_clean\",\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "# Drop rows without category (should not happen)\n",
        "esc_merged = esc_merged.dropna(subset=[\"category\"])\n",
        "\n",
        "# LOWERCASE for filtering\n",
        "esc_merged[\"category_lc\"] = esc_merged[\"category\"].str.lower()\n",
        "\n",
        "# Filter OUT insect-like ESC categories\n",
        "mask_env = ~esc_merged[\"category_lc\"].apply(\n",
        "    lambda x: any(kw in x for kw in ESC_INSECT_KEYWORDS)\n",
        ")\n",
        "\n",
        "esc_env = esc_merged[mask_env].copy()\n",
        "esc_env[\"label\"] = \"Env\"\n",
        "\n",
        "print(\"ESC cleaned rows kept:\", esc_env.shape[0])\n",
        "\n",
        "esc_env_out = os.path.join(OUTPUT_DIR, \"esc50_env_cleaned.csv\")\n",
        "esc_env.to_csv(esc_env_out, index=False)\n",
        "print(\"Saved:\", esc_env_out)\n",
        "\n",
        "# -------------------------\n",
        "# RELABEL INSECT459\n",
        "# -------------------------\n",
        "print(\"\\nRelabeling insect459...\")\n",
        "\n",
        "df_insect459[\"orig_label\"] = df_insect459[\"label\"]\n",
        "\n",
        "df_insect459[\"label\"] = df_insect459[\"orig_label\"].apply(\n",
        "    lambda x: x if x in TOP3 else \"Others\"\n",
        ")\n",
        "\n",
        "print(df_insect459[\"label\"].value_counts())\n",
        "\n",
        "insect_out = os.path.join(OUTPUT_DIR, \"insect459_relabeled.csv\")\n",
        "df_insect459.to_csv(insect_out, index=False)\n",
        "print(\"Saved:\", insect_out)\n",
        "\n",
        "# -------------------------\n",
        "# RELABEL XENOCANTO\n",
        "# -------------------------\n",
        "print(\"\\nRelabeling xenocanto...\")\n",
        "\n",
        "def relabel_xeno(row):\n",
        "    f = str(row[\"file\"]).lower()\n",
        "    l = str(row[\"label\"]).lower()\n",
        "    for sp in TOP3:\n",
        "        if sp.lower() in f or sp.lower() in l:\n",
        "            return sp\n",
        "    return \"Others\"\n",
        "\n",
        "df_xeno[\"orig_label\"] = df_xeno[\"label\"]\n",
        "df_xeno[\"label\"] = df_xeno.apply(relabel_xeno, axis=1)\n",
        "\n",
        "print(df_xeno[\"label\"].value_counts())\n",
        "\n",
        "xeno_out = os.path.join(OUTPUT_DIR, \"xeno_relabeled.csv\")\n",
        "df_xeno.to_csv(xeno_out, index=False)\n",
        "print(\"Saved:\", xeno_out)\n",
        "\n",
        "# -------------------------\n",
        "# ALIGN COLUMNS AND MERGE\n",
        "# -------------------------\n",
        "print(\"\\nMerging datasets...\")\n",
        "\n",
        "dfs = [df_insect459, df_xeno, esc_env]\n",
        "\n",
        "# Identify common feature columns\n",
        "common_cols = set(dfs[0].columns)\n",
        "for d in dfs:\n",
        "    common_cols = common_cols.intersection(d.columns)\n",
        "\n",
        "common_cols = sorted(list(common_cols))\n",
        "\n",
        "print(\"Common cols:\", len(common_cols))\n",
        "\n",
        "# Keep only those columns\n",
        "dfs = [d[common_cols] for d in dfs]\n",
        "\n",
        "final = pd.concat(dfs, ignore_index=True)\n",
        "print(\"Final merged shape:\", final.shape)\n",
        "print(\"Final label distribution:\\n\", final[\"label\"].value_counts())\n",
        "\n",
        "final_out = os.path.join(OUTPUT_DIR, \"final_insect_env_merged.csv\")\n",
        "final.to_csv(final_out, index=False)\n",
        "print(\"Saved final merged dataset:\", final_out)\n",
        "\n",
        "print(\"\\nDONE.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e4eceda-cf93-42cf-9010-b8291a7431a0",
      "metadata": {
        "id": "7e4eceda-cf93-42cf-9010-b8291a7431a0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "# ----------------------------\n",
        "# CONFIG\n",
        "# ----------------------------\n",
        "CSV_PATH = r\"D:\\Projects\\Minor\\Audio\\InsectsXeno\\Audio\\merged_output\\final_insect_env_merged.csv\"\n",
        "N_FOLDS = 5\n",
        "SEED = 42\n",
        "\n",
        "# ----------------------------\n",
        "# LOAD DATA\n",
        "# ----------------------------\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "print(\"Loaded:\", df.shape)\n",
        "\n",
        "# separate features and labels\n",
        "X = df.drop(columns=[\"label\", \"file\"], errors=\"ignore\")\n",
        "y = df[\"label\"]\n",
        "\n",
        "# encode labels\n",
        "le = LabelEncoder()\n",
        "y_enc = le.fit_transform(y)\n",
        "\n",
        "print(\"Classes:\", list(le.classes_))\n",
        "\n",
        "# ----------------------------\n",
        "# K-Fold Cross-Validation\n",
        "# ----------------------------\n",
        "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
        "\n",
        "fold_accuracies = []\n",
        "\n",
        "print(\"\\n========== CROSS VALIDATION ==========\")\n",
        "\n",
        "for fold, (train_idx, test_idx) in enumerate(skf.split(X, y_enc), 1):\n",
        "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
        "    y_train, y_test = y_enc[train_idx], y_enc[test_idx]\n",
        "\n",
        "    model = LGBMClassifier(\n",
        "        n_estimators=300,\n",
        "        learning_rate=0.05,\n",
        "        num_leaves=55,\n",
        "        max_depth=-1,\n",
        "        random_state=SEED,\n",
        "        class_weight=\"balanced\"  # important for imbalance\n",
        "    )\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    preds = model.predict(X_test)\n",
        "    acc = accuracy_score(y_test, preds)\n",
        "    fold_accuracies.append(acc)\n",
        "\n",
        "    print(f\"\\n--- Fold {fold} Accuracy: {acc:.4f} ---\")\n",
        "    print(classification_report(y_test, preds, target_names=le.classes_))\n",
        "\n",
        "print(\"\\n========== FINAL RESULTS ==========\")\n",
        "print(\"Fold Accuracies:\", np.round(fold_accuracies, 4))\n",
        "print(\"Mean Accuracy:\", np.mean(fold_accuracies))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3e70b16-21f2-440a-bc82-70806e8befcd",
      "metadata": {
        "id": "c3e70b16-21f2-440a-bc82-70806e8befcd"
      },
      "outputs": [],
      "source": [
        "pip install lightgbm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b90b47af-bed4-42cf-ba91-51a29f9f48fc",
      "metadata": {
        "id": "b90b47af-bed4-42cf-ba91-51a29f9f48fc"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Ultra-fast Mel Extractor v3 (Fixed for nested folders + Windows)\n",
        " - GPU batched mel-spectrogram extraction\n",
        " - Multiprocess audio loading + segmentation\n",
        " - Correctly detects ALL nested subfolder audio files\n",
        " - Save .pt tensors per segment (resume-safe)\n",
        "Requirements:\n",
        "  pip install torch torchaudio soundfile pydub tqdm\n",
        "  (ffmpeg required for non-wav files)\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import math\n",
        "import time\n",
        "import glob\n",
        "import random\n",
        "import argparse\n",
        "import multiprocessing as mp\n",
        "from pathlib import Path\n",
        "from functools import partial\n",
        "from collections import Counter\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchaudio\n",
        "import soundfile as sf\n",
        "from pydub import AudioSegment\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# ---------------- CONFIG ----------------\n",
        "INPUT_FOLDERS = [\n",
        "    r\"E:\\Insect459\\Train\\Train\",\n",
        "    r\"D:\\Projects\\Minor\\Audio\\InsectsXeno\\Audio\",\n",
        "    r\"C:\\Users\\DEV\\OneDrive\\Desktop\\Macaulay\\Macaulary\\macaulay_categorized\",\n",
        "]\n",
        "\n",
        "OUTPUT_DIR = r\"E:\\MelSpectros\\fast_mels_v3\"\n",
        "\n",
        "SAMPLE_RATE = 22050\n",
        "N_MELS = 128\n",
        "N_FFT = 2048\n",
        "HOP_LENGTH = 512\n",
        "FMIN = 50\n",
        "FMAX = 12000\n",
        "\n",
        "SEGMENT_DURATION = 3.0\n",
        "NUM_SEGMENTS = 3\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "USE_FP16 = True\n",
        "ENABLE_LOGMEL = True\n",
        "NORMALIZE_PER_SPEC = True\n",
        "NUM_WORKERS = min(8, mp.cpu_count() - 1)\n",
        "ALLOWED_EXTS = (\".wav\", \".mp3\", \".flac\", \".m4a\", \".ogg\", \".aiff\", \".aif\")\n",
        "# ----------------------------------------\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "DTYPE = torch.float16 if (USE_FP16 and DEVICE.type == \"cuda\") else torch.float32\n",
        "\n",
        "\n",
        "# ------- FIXED: Nested-folder collector -------\n",
        "def collect_files(input_folders, allowed_exts):\n",
        "    collected = []\n",
        "    allowed = tuple([e.lower() for e in allowed_exts])\n",
        "\n",
        "    for folder in input_folders:\n",
        "        folder_abs = os.path.abspath(folder)\n",
        "        if not os.path.isdir(folder_abs):\n",
        "            print(f\"[WARNING] Not a directory: {folder_abs}\")\n",
        "            continue\n",
        "\n",
        "        for root, _, files in os.walk(folder_abs):\n",
        "            for f in files:\n",
        "                if f.lower().endswith(allowed):\n",
        "                    full_path = os.path.join(root, f)\n",
        "                    rel_path = os.path.relpath(full_path, folder_abs)\n",
        "                    collected.append((folder_abs, rel_path))\n",
        "\n",
        "    return collected\n",
        "\n",
        "\n",
        "# ---------------- AUDIO LOADING ----------------\n",
        "def load_audio_file(path, target_sr):\n",
        "    ext = Path(path).suffix.lower()\n",
        "\n",
        "    # Try soundfile ‚Üí torchaudio ‚Üí pydub fallback\n",
        "    try:\n",
        "        y, sr = sf.read(path, dtype='float32')\n",
        "        if y.ndim > 1:\n",
        "            y = y.mean(axis=1)\n",
        "        if sr != target_sr:\n",
        "            wav = torch.from_numpy(y).unsqueeze(0)\n",
        "            wav = torchaudio.transforms.Resample(sr, target_sr)(wav)\n",
        "            y = wav.squeeze(0).numpy()\n",
        "        return y.astype(np.float32), target_sr\n",
        "\n",
        "    except Exception:\n",
        "        try:\n",
        "            wav, sr = torchaudio.load(path)\n",
        "            wav = wav.mean(dim=0).numpy() if wav.ndim > 1 else wav.numpy()\n",
        "            if sr != target_sr:\n",
        "                wav_t = torch.from_numpy(wav).unsqueeze(0)\n",
        "                wav_t = torchaudio.transforms.Resample(sr, target_sr)(wav_t)\n",
        "                wav = wav_t.squeeze(0).numpy()\n",
        "            return wav.astype(np.float32), target_sr\n",
        "\n",
        "        except Exception:\n",
        "            audio = AudioSegment.from_file(path)\n",
        "            audio = audio.set_frame_rate(target_sr).set_channels(1).set_sample_width(2)\n",
        "            samples = np.array(audio.get_array_of_samples()).astype(np.float32) / (2**15)\n",
        "            return samples.astype(np.float32), target_sr\n",
        "\n",
        "\n",
        "# ---------------- SEGMENT LOGIC ----------------\n",
        "def generate_segment_starts(total_len, seg_len, num_segments):\n",
        "    if total_len <= seg_len:\n",
        "        return [0]\n",
        "\n",
        "    max_start = total_len - seg_len\n",
        "\n",
        "    if num_segments == 1:\n",
        "        return [random.randint(0, max_start)]\n",
        "\n",
        "    starts = set()\n",
        "    while len(starts) < num_segments:\n",
        "        starts.add(random.randint(0, max_start))\n",
        "\n",
        "    return sorted(list(starts))[:num_segments]\n",
        "\n",
        "\n",
        "# ------------ WORKER: load + segment --------------\n",
        "def worker_load_and_segment(root_rel, seg_len_samples, num_segments, sample_rate):\n",
        "    root, rel_path = root_rel\n",
        "    full_path = os.path.join(root, rel_path)\n",
        "\n",
        "    # Label = top folder name\n",
        "    label = os.path.basename(root)\n",
        "\n",
        "    basename = Path(rel_path).stem\n",
        "\n",
        "    try:\n",
        "        y, sr = load_audio_file(full_path, sample_rate)\n",
        "    except Exception:\n",
        "        return []\n",
        "\n",
        "    if len(y) == 0:\n",
        "        return []\n",
        "\n",
        "    starts = generate_segment_starts(len(y), seg_len_samples, num_segments)\n",
        "    out = []\n",
        "\n",
        "    for i, s in enumerate(starts, start=1):\n",
        "        seg = y[s:s+seg_len_samples]\n",
        "        if len(seg) < seg_len_samples:\n",
        "            seg = np.pad(seg, (0, seg_len_samples - len(seg)))\n",
        "\n",
        "        key = f\"{basename}_seg{i}\"\n",
        "        out.append((label, basename, key, seg.astype(np.float32)))\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "# ---------- SAVE TENSOR ----------\n",
        "def save_tensor(tensor, out_path):\n",
        "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
        "    torch.save(tensor.cpu(), out_path)\n",
        "\n",
        "\n",
        "# ---------- PROCESS & SAVE BATCH ----------\n",
        "def process_and_save_batch(batch_seg_arrays, batch_meta, mel_transform, amp_to_db, spect_dir):\n",
        "    wav_batch = torch.from_numpy(np.stack(batch_seg_arrays)).to(DEVICE).unsqueeze(1)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        mels = mel_transform(wav_batch)\n",
        "\n",
        "        if ENABLE_LOGMEL:\n",
        "            mels = amp_to_db(mels)\n",
        "\n",
        "        mels = mels.to(DTYPE)\n",
        "\n",
        "    for i, meta in enumerate(batch_meta):\n",
        "        label, basename, key = meta\n",
        "\n",
        "        out_dir = spect_dir / label\n",
        "        out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        out_path = out_dir / f\"{key}.pt\"\n",
        "\n",
        "        spec = mels[i]\n",
        "\n",
        "        if NORMALIZE_PER_SPEC:\n",
        "            mn = float(spec.min().cpu())\n",
        "            mx = float(spec.max().cpu())\n",
        "            if mx - mn > 1e-6:\n",
        "                spec = (spec - mn) / (mx - mn)\n",
        "\n",
        "        save_tensor(spec, str(out_path))\n",
        "\n",
        "\n",
        "# =================== MAIN ===================\n",
        "def main():\n",
        "    print(\"Device:\", DEVICE, \"dtype:\", DTYPE)\n",
        "\n",
        "    seg_len_samples = int(SEGMENT_DURATION * SAMPLE_RATE)\n",
        "    mel_transform = torchaudio.transforms.MelSpectrogram(\n",
        "        sample_rate=SAMPLE_RATE,\n",
        "        n_fft=N_FFT,\n",
        "        hop_length=HOP_LENGTH,\n",
        "        n_mels=N_MELS,\n",
        "        f_min=FMIN,\n",
        "        f_max=min(FMAX, SAMPLE_RATE // 2),\n",
        "        power=2.0,\n",
        "        norm='slaney',\n",
        "        mel_scale='htk'\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    amp_to_db = torchaudio.transforms.AmplitudeToDB(stype='power').to(DEVICE)\n",
        "\n",
        "    # -------- NEW FIXED COLLECTOR --------\n",
        "    file_list = collect_files(INPUT_FOLDERS, ALLOWED_EXTS)\n",
        "\n",
        "    print(\"Files found:\", len(file_list))\n",
        "    print(\"Estimated segments:\", len(file_list) * NUM_SEGMENTS)\n",
        "\n",
        "    if len(file_list) == 0:\n",
        "        print(\"No audio files found!\")\n",
        "        return\n",
        "\n",
        "    # -------- Make output dirs --------\n",
        "    spect_dir = Path(OUTPUT_DIR) / \"spectrograms\"\n",
        "    spect_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # -------- Windows-safe multiprocessing --------\n",
        "    ctx = mp.get_context(\"spawn\")\n",
        "    pool = ctx.Pool(processes=NUM_WORKERS)\n",
        "\n",
        "    loader_fn = partial(\n",
        "        worker_load_and_segment,\n",
        "        seg_len_samples=seg_len_samples,\n",
        "        num_segments=NUM_SEGMENTS,\n",
        "        sample_rate=SAMPLE_RATE\n",
        "    )\n",
        "\n",
        "    batch_seg_arrays = []\n",
        "    batch_meta = []\n",
        "    processed = 0\n",
        "    failed = 0\n",
        "\n",
        "    pbar = tqdm(total=len(file_list), desc=\"Loading & segmenting files\")\n",
        "\n",
        "    for file_segments in pool.imap_unordered(loader_fn, file_list, chunksize=1):\n",
        "        pbar.update(1)\n",
        "\n",
        "        if not file_segments:\n",
        "            failed += 1\n",
        "            continue\n",
        "\n",
        "        for label, basename, key, seg in file_segments:\n",
        "\n",
        "            out_path = spect_dir / label / f\"{key}.pt\"\n",
        "            if out_path.exists():\n",
        "                continue\n",
        "\n",
        "            batch_seg_arrays.append(seg)\n",
        "            batch_meta.append((label, basename, key))\n",
        "\n",
        "            if len(batch_seg_arrays) >= BATCH_SIZE:\n",
        "                process_and_save_batch(batch_seg_arrays, batch_meta, mel_transform, amp_to_db, spect_dir)\n",
        "                processed += len(batch_seg_arrays)\n",
        "                batch_seg_arrays = []\n",
        "                batch_meta = []\n",
        "\n",
        "    pbar.close()\n",
        "    pool.close()\n",
        "    pool.join()\n",
        "\n",
        "    if batch_seg_arrays:\n",
        "        process_and_save_batch(batch_seg_arrays, batch_meta, mel_transform, amp_to_db, spect_dir)\n",
        "        processed += len(batch_seg_arrays)\n",
        "\n",
        "    print(f\"\\nDone. Processed: {processed} segments, Failed files: {failed}\")\n",
        "    print(\"Saved to:\", spect_dir)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    mp.freeze_support()\n",
        "    start = time.time()\n",
        "    main()\n",
        "    print(\"Total time (s):\", time.time() - start)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}